#!/usr/bin/env python3
"""
RepoDetailAnalyzer

This script analyzes repositories under specified project keys (SPKs) using the Bitbucket API.
For each repository, it collects:
  - Repository metadata: creation date and last update date.
  - Top branch (most recently modified branch).
  - Branch count.
  - Commit count.
  - File listing (without reading file content) to infer:
        • Languages (via file extensions)
        • Frameworks/technologies (via known filenames and folder heuristics)
  - Total file count.

It uses rate limiting (via a TokenBucket), structured JSON logging, and parallel processing.
"""

import os
import time
import json
import random
import logging
import threading
import requests
import openpyxl
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------------------------
# Custom JSON Logging Formatter
# ------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

def configure_logging(logs_folder: str) -> logging.Logger:
    os.makedirs(logs_folder, exist_ok=True)
    log_path = os.path.join(logs_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    return logging.getLogger("RepoDetailAnalyzer")

logger = configure_logging("logs")

# ------------------------------
# Data Sanitization Utilities
# ------------------------------
def sanitize_map_objects(value):
    if isinstance(value, map):
        return list(value)
    return value

def sanitize_rows(rows):
    return [{k: sanitize_map_objects(v) for k, v in row.items()} for row in rows]

# ------------------------------
# Token Bucket for Rate Limiting
# ------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float) -> None:
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1) -> None:
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ------------------------------
# Generic API Client with Rate Limiting & Exponential Backoff
# ------------------------------
class GenericAPIClient:
    def __init__(self, base_url: str, token_bucket: TokenBucket, auth: tuple = None):
        self.base_url = base_url.rstrip("/")  # remove trailing slash if any
        self.token_bucket = token_bucket
        self.auth = auth
        self.session = requests.Session()
        if self.auth:
            self.session.auth = self.auth
        # Adjust SSL verification and adapter settings as needed.
        self.session.verify = False
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.logger = logging.getLogger("GenericAPIClient")

    def make_request(self, endpoint: str, params: dict = None, retries: int = 3, base_delay: float = 1.0):
        full_url = f"{self.base_url}{endpoint}"
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {full_url} (Attempt {attempt+1}/{retries})")
                response = self.session.get(full_url, params=params)
                response.raise_for_status()
                return response.json(), None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                status_code = response.status_code if response else "N/A"
                self.logger.error(
                    f"Error for {full_url}: {error_message} (Status: {status_code})",
                    extra={"error_category": "API Failure", "severity": "error"}
                )
                sleep_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                time.sleep(sleep_time)
        return None, "Max retries exceeded"

# ------------------------------
# Mapping for File Extensions to Languages
# ------------------------------
EXT_LANG_MAP = {
    ".py": "Python",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
    ".rb": "Ruby",
    ".cs": "C#",
    ".cpp": "C++",
    ".c": "C",
    ".go": "Go",
    ".php": "PHP",
    ".rs": "Rust",
    ".kt": "Kotlin",
    ".swift": "Swift",
    # Extend this mapping as needed.
}

# ------------------------------
# Mapping for Specific Files to Frameworks/Technologies
# ------------------------------
FILE_FRAMEWORK_MAP = {
    "pom.xml": "Maven",
    "build.gradle": "Gradle",
    "package.json": "Node.js / npm",
    "requirements.txt": "Python (pip)",
    "Gemfile": "Ruby on Rails",
    "composer.json": "Composer (PHP)",
    "Cargo.toml": "Rust (Cargo)",
    "Makefile": "Make",
    # Extend mapping as needed.
}

# ------------------------------
# Bitbucket API Interaction Functions
# ------------------------------

def get_repos(api_client: GenericAPIClient, project_key: str) -> list:
    """
    Retrieves all repositories for the given project (SPK).
    """
    endpoint = f"/projects/{project_key}/repos"
    params = {"limit": 1000}
    data, error = api_client.make_request(endpoint, params=params)
    if error:
        logger.error(
            f"Failed to get repos for project {project_key}: {error}",
            extra={"error_category": "API Failure", "severity": "error"}
        )
        return []
    return data.get("values", [])

def get_repo_details(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> tuple:
    """
    Retrieves repository details such as creation and last update dates.
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}"
    data, error = api_client.make_request(endpoint)
    return data, error

def get_top_branch(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> str:
    """
    Retrieves the top (most recently modified) branch for the repository.
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/branches"
    params = {"orderBy": "MODIFICATION", "limit": 1}
    data, error = api_client.make_request(endpoint, params=params)
    if error:
        logger.error(
            f"Failed to get top branch for {project_key}/{repo_slug}: {error}",
            extra={"error_category": "API Failure", "severity": "error"}
        )
        return None
    branches = data.get("values", [])
    if branches:
        return branches[0].get("displayId")
    return None

def get_branches(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> tuple:
    """
    Retrieves branches and returns the branch count.
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/branches"
    params = {"limit": 100}
    data, error = api_client.make_request(endpoint, params=params)
    return data, error

def get_commit_count(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> tuple:
    """
    Retrieves commit count using the commits endpoint.
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/commits"
    params = {"limit": 1}
    data, error = api_client.make_request(endpoint, params=params)
    return data, error

def get_repo_files(api_client: GenericAPIClient, project_key: str, repo_slug: str, branch: str) -> list:
    """
    Retrieves a list of file paths for the repository on the given branch.
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/files"
    params = {"at": branch}
    data, error = api_client.make_request(endpoint, params=params)
    if error:
        logger.error(
            f"Failed to retrieve files for {project_key}/{repo_slug} at branch {branch}: {error}",
            extra={"error_category": "API Failure", "severity": "error"}
        )
        return []
    files = data.get("files")
    if files is None:
        # Some endpoints may return a list directly.
        files = data if isinstance(data, list) else []
    return files

# ------------------------------
# File Analysis Functions
# ------------------------------
def analyze_file_list(file_list: list) -> dict:
    """
    Analyzes the file list to determine languages (via file extensions)
    and frameworks/technologies (via specific file names or folder hints).
    Returns a dictionary with:
      - languages: list of unique languages detected
      - frameworks: list of unique frameworks/technologies detected
      - file_count: total number of files
    """
    languages = set()
    frameworks = set()
    for file_path in file_list:
        base_name = os.path.basename(file_path)
        ext = os.path.splitext(base_name)[1].lower()
        if ext in EXT_LANG_MAP:
            languages.add(EXT_LANG_MAP[ext])
        if base_name in FILE_FRAMEWORK_MAP:
            frameworks.add(FILE_FRAMEWORK_MAP[base_name])
        # Folder structure heuristics:
        if "node_modules" in file_path:
            frameworks.add("Node.js")
        if "venv" in file_path or "env" in file_path:
            frameworks.add("Python VirtualEnv")
        if "src/main/java" in file_path:
            languages.add("Java")
        if "src/main/resources" in file_path:
            frameworks.add("Spring")
        # Extend with additional heuristics as needed.
    return {
        "languages": list(languages),
        "frameworks": list(frameworks),
        "file_count": len(file_list)
    }

# ------------------------------
# Comprehensive Repository Analysis
# ------------------------------
def analyze_repository(api_client: GenericAPIClient, project_key: str, repo: dict) -> dict:
    """
    For a given repository, gather:
      - Repository details (creation & update dates)
      - Top branch
      - Branch count
      - Commit count
      - File listing analysis (languages, frameworks, file count)
    Returns a dictionary with all these details.
    """
    repo_slug = repo.get("slug")
    repo_name = repo.get("name", repo_slug)
    logger.info(f"Analyzing repository {project_key}/{repo_slug}")
    
    # Repository metadata (creation/update dates)
    repo_details, err_details = get_repo_details(api_client, project_key, repo_slug)
    if err_details:
        logger.error(f"Error fetching details for {project_key}/{repo_slug}: {err_details}")
    created_date = repo_details.get("createdDate") if repo_details else None
    updated_date = repo_details.get("updatedDate") if repo_details else None

    # Top branch
    top_branch = get_top_branch(api_client, project_key, repo_slug)
    if not top_branch:
        logger.error(f"Skipping repository {project_key}/{repo_slug} due to missing top branch.")
        return None

    # Branch count
    branches_data, err_branches = get_branches(api_client, project_key, repo_slug)
    branch_count = branches_data.get("size") if branches_data and "size" in branches_data else None

    # Commit count
    commit_data, err_commits = get_commit_count(api_client, project_key, repo_slug)
    commit_count = commit_data.get("size") if commit_data and "size" in commit_data else None

    # File listing
    file_list = get_repo_files(api_client, project_key, repo_slug, top_branch)
    file_analysis = analyze_file_list(file_list)

    result = {
        "project_key": project_key,
        "repo_slug": repo_slug,
        "repo_name": repo_name,
        "top_branch": top_branch,
        "created_date": created_date,
        "updated_date": updated_date,
        "branch_count": branch_count,
        "commit_count": commit_count,
        "languages": file_analysis.get("languages", []),
        "frameworks": file_analysis.get("frameworks", []),
        "file_count": file_analysis.get("file_count", 0)
    }
    logger.info(f"Completed analysis for {project_key}/{repo_slug}: {result}")
    return result

# ------------------------------
# Project Processing Functions
# ------------------------------
def process_single_project(api_client: GenericAPIClient, project_key: str) -> list:
    """
    Processes a single project (SPK): retrieves all repos and analyzes each one.
    Returns a list of analysis results for that project.
    """
    repos = get_repos(api_client, project_key)
    project_results = []
    for repo in repos:
        result = analyze_repository(api_client, project_key, repo)
        if result:
            project_results.append(result)
    return project_results

def process_spks(input_excel: str, api_client: GenericAPIClient) -> list:
    """
    Reads SPKs (project keys) from an Excel file, processes each project in parallel,
    and returns a list of repository analysis results.
    """
    wb = openpyxl.load_workbook(input_excel)
    sheet = wb.active
    # Assumes that SPKs are in the first column starting from row 2.
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    all_results = []

    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_project = {}
        for row in spk_rows:
            project_key = row[0]
            if not project_key:
                continue
            future = executor.submit(process_single_project, api_client, project_key)
            future_to_project[future] = project_key

        for future in as_completed(future_to_project):
            project_key = future_to_project[future]
            results = future.result()
            if results:
                all_results.extend(results)
            logger.info(f"Completed processing project {project_key}")

    return all_results

# ------------------------------
# Main Entry Point
# ------------------------------
def main():
    start_time = datetime.now()
    logger.info("RepoDetailAnalyzer process started.")
    
    # Path to the Excel file containing SPKs (project keys)
    input_excel = "WTMSPK.xlsx"
    
    # Bitbucket API configuration:
    BITBUCKET_BASE_URL = os.getenv("BITBUCKET_BASE_URL", "https://scm.example.com/rest/api/1.0")
    BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
    BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")
    
    token_bucket = TokenBucket(capacity=75, tokens_per_second=5)
    api_client = GenericAPIClient(BITBUCKET_BASE_URL, token_bucket, auth=(BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD))
    
    results = process_spks(input_excel, api_client)
    
    # Sanitize and output results.
    sanitized = sanitize_rows(results)
    logger.info(f"Total repositories analyzed: {len(sanitized)}")
    
    # For demonstration, print the results in JSON format.
    for item in sanitized:
        print(json.dumps(item, indent=2))
    
    end_time = datetime.now()
    logger.info(f"Process completed in {str(end_time - start_time)}.")

if __name__ == "__main__":
    main()
