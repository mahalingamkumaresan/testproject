import re
import threading
import logging
import json
import time
import os
import pandas as pd
import requests
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from openpyxl import load_workbook
from requests.adapters import HTTPAdapter

# ------------------------------
# Custom JSON Logging Formatter
# ------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

# ------------------------------
# Checkpoint Utilities
# ------------------------------
def load_checkpoint(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def update_checkpoint(file_path, identifier):
    with open(file_path, 'a') as f:
        f.write(f"{identifier}\n")

# ------------------------------
# Token Bucket for Rate Limiting with Consumption Tracking
# ------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float):
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1):
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ------------------------------
# Bitbucket API Wrapper with Session Reuse and Increased Connection Pool
# ------------------------------
class BitbucketAPI:
    def __init__(self, base_url: str, username: str, app_password: str, token_bucket: TokenBucket):
        self.base_url = base_url
        self.username = username
        self.app_password = app_password
        self.token_bucket = token_bucket
        self.session = requests.Session()
        self.session.auth = (self.username, self.app_password)
        self.session.verify = False
        # Increase connection pool size to 50
        adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        self.logger = logging.getLogger("BitbucketAPI")

    def make_request(self, url: str, retries: int = 2, delay: int = 2):
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {url} (Attempt {attempt+1}/{retries})")
                response = self.session.get(url)
                response.raise_for_status()
                return response, None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                # Include status code if available
                status_code = response.status_code if response is not None else "N/A"
                extra = {"error_category": "API Failure", "severity": "error"}
                self.logger.error(f"Request failed for {url}: {error_message} (Status: {status_code})", extra=extra)
                if response is not None and response.status_code in [404, 401]:
                    try:
                        error_message = response.json().get('errors', [{}])[0].get('message', error_message)
                        return None, error_message
                    except Exception:
                        pass
                if attempt < retries - 1:
                    time.sleep(delay)
                if response is not None and response.status_code == 429:
                    time.sleep(delay * 10)
                else:
                    return None, error_message
        return None, "Max retries exceeded"

# ------------------------------
# Helper Function: Diff Parsing from API Response
# ------------------------------
def parse_diff_change(change: dict) -> (int, int, int):
    lines_added = 0
    lines_removed = 0
    lines_modified = 0  # Some APIs donâ€™t provide a separate modified count.
    try:
        for hunk in change.get('hunks', []):
            for segment in hunk.get('segments', []):
                if segment.get('type') == 'REMOVED':
                    for _ in segment.get('lines', []):
                        lines_removed += 1
                elif segment.get('type') == 'ADDED':
                    for _ in segment.get('lines', []):
                        lines_added += 1
    except Exception as e:
        logging.getLogger("DiffParser").error(f"Error parsing diff change: {e}")
        raise e
    return lines_added, lines_removed, lines_modified

# ------------------------------
# Stub for Future Complexity/Commit Details Logic
# ------------------------------
def calculate_complexity_and_details(change: dict) -> (int, dict):
    return 0, {}

# ------------------------------
# Commit Processor: API-Only Approach for SPK Mode
# ------------------------------
class CommitProcessor:
    def __init__(self, api: BitbucketAPI, output_folder: str, author_email_filter: set = None, recent_branches: int = 1):
        # recent_branches fixed to 1 per requirement.
        self.api = api
        self.output_folder = output_folder
        self.author_email_filter = author_email_filter
        self.recent_branches = recent_branches
        self.logger = logging.getLogger("CommitProcessor")
        self.jira_pattern = r"\b[A-Z]+-\d+\b"
        self.failed_urls = []
        self.diff_failures = []

    def get_top_branch(self, project_key: str, repo_slug: str, start_date: str, end_date: str):
        branch_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/branches?limit=1000"
        response, err = self.api.make_request(branch_url)
        if not response:
            self.failed_urls.append({
                "url": branch_url,
                "reason": err,
                "error_category": "API Failure",
                "severity": "error"
            })
            return None
        branches_data = response.json().get("values", [])
        branches_with_timestamp = []
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        for branch in branches_data:
            branch_name = branch.get("displayId")
            latest_commit = branch.get("latestCommit")
            if not branch_name or not latest_commit:
                continue
            commit_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{latest_commit}"
            commit_resp, commit_err = self.api.make_request(commit_url)
            if not commit_resp:
                self.failed_urls.append({
                    "url": commit_url,
                    "reason": commit_err,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                continue
            commit_details = commit_resp.json()
            timestamp = commit_details.get("authorTimestamp")
            if timestamp:
                commit_time = datetime.fromtimestamp(timestamp / 1000)
                if start_dt <= commit_time <= end_dt:
                    branches_with_timestamp.append({
                        "branch": branch_name,
                        "timestamp": timestamp
                    })
        if not branches_with_timestamp:
            self.logger.debug(f"No branches found for {project_key}/{repo_slug} within date range")
            return None
        top_branch = sorted(branches_with_timestamp, key=lambda x: x["timestamp"], reverse=True)[0]["branch"]
        self.logger.debug(f"Top branch for {project_key}/{repo_slug} within date range: {top_branch}")
        return top_branch

    def process_commits_spk(self, project_key: str, repo_slug: str, start_date: str, end_date: str):
        deduped_commits = {}
        top_branch = self.get_top_branch(project_key, repo_slug, start_date, end_date)
        if not top_branch:
            return []
        url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits?limit=1000&until={top_branch}"
        response, error_reason = self.api.make_request(url)
        if not response:
            self.failed_urls.append({
                "url": url,
                "reason": error_reason,
                "error_category": "API Failure",
                "severity": "error"
            })
            return []
        data = response.json()
        for commit in data.get('values', []):
            commit_date = datetime.fromtimestamp(commit["authorTimestamp"] / 1000)
            commit_month = commit_date.strftime('%Y-%m')
            start_month = datetime.strptime(start_date, '%Y-%m-%d').strftime('%Y-%m')
            end_month = datetime.strptime(end_date, '%Y-%m-%d').strftime('%Y-%m')
            if not (start_month <= commit_month <= end_month):
                continue
            author_email = (commit["author"].get('emailAddress') or commit["author"].get('emailaddress') or '').lower()
            if self.author_email_filter and author_email not in self.author_email_filter:
                continue
            commit_id = commit.get('id', '')
            if commit_id not in deduped_commits:
                deduped_commits[commit_id] = commit
        output_rows = []
        for commit in deduped_commits.values():
            commit_date = datetime.fromtimestamp(commit["authorTimestamp"] / 1000)
            commit_month = commit_date.strftime('%Y-%m')
            author_name = (commit["author"].get('displayName') or 
                           commit["author"].get('displayname') or 
                           commit["author"].get('name') or 'Unknown Author')
            author_email = (commit["author"].get('emailAddress') or 
                            commit["author"].get('emailaddress') or '').lower()
            commit_id = commit.get('id', '')
            self.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug}")
            diff_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
            file_response, diff_error = self.api.make_request(diff_url)
            if not file_response:
                self.failed_urls.append({
                    "url": diff_url,
                    "reason": diff_error,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                continue
            try:
                diff_data = file_response.json()
            except Exception as e:
                self.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": str(e),
                    "error_category": "Diff Parsing Failure",
                    "severity": "error"
                })
                continue
            first_row = True
            if 'diffs' not in diff_data or not diff_data['diffs']:
                self.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": "No diff data found",
                    "error_category": "Diff Parsing Failure",
                    "severity": "warning"
                })
                continue
            for change in diff_data.get("diffs", []):
                source_info = change.get('source', {})
                dest_info = change.get('destination', {})
                file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                             else source_info.get('name', 'Unknown'))
                file_status = "modified"
                if not source_info:
                    file_status = "new"
                elif not dest_info:
                    file_status = "deleted"
                try:
                    added, removed, modified = parse_diff_change(change)
                    if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                        self.diff_failures.append({
                            "url": diff_url,
                            "commit_id": commit_id,
                            "reason": "No changes found in diff",
                            "error_category": "Diff Parsing Failure",
                            "severity": "warning"
                        })
                except Exception as e:
                    self.diff_failures.append({
                        "url": diff_url,
                        "commit_id": commit_id,
                        "reason": f"Exception during diff parsing: {e}",
                        "error_category": "Diff Parsing Failure",
                        "severity": "error"
                    })
                    continue
                row = {
                    "Project Key": project_key,
                    "Repo Slug": repo_slug,
                    "Author Name": author_name,
                    "Author Email": author_email,
                    "Month": commit_month,
                    "File Name": file_name,
                    "Month Lines Added": added,
                    "Month Lines Removed": removed,
                    "Month Lines Modified": modified,
                    "Total Commits": 1 if first_row else 0,
                    "Commit Id": commit_id,
                    "File Status": file_status
                }
                output_rows.append(row)
                first_row = False
        return output_rows

    def process_commits_by_ids(self, commit_id_file: str):
        # For commit mode (unchanged)
        df = pd.read_excel(commit_id_file)
        checkpoint_file = os.path.join(self.output_folder, "commit_checkpoint.txt")
        processed_commits = load_checkpoint(checkpoint_file)
        df = df[~df["Commit Id"].isin(processed_commits)]
        output_rows = []
        for index, row in df.iterrows():
            commit_id = row["Commit Id"]
            project_key = row["Project Key"]
            repo_slug = row["Repo Slug"]
            self.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug} (API mode)")
            diff_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
            file_response, diff_error = self.api.make_request(diff_url)
            if not file_response:
                self.failed_urls.append({
                    "url": diff_url,
                    "reason": diff_error,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                continue
            try:
                diff_data = file_response.json()
            except Exception as e:
                self.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": str(e),
                    "error_category": "Diff Parsing Failure",
                    "severity": "error"
                })
                continue
            commit_month = "Unknown"
            if "authorTimestamp" in row:
                try:
                    commit_date = datetime.fromtimestamp(float(row["authorTimestamp"]) / 1000)
                    commit_month = commit_date.strftime('%Y-%m')
                except Exception:
                    commit_month = "Unknown"
            first_row = True
            if 'diffs' not in diff_data or not diff_data['diffs']:
                self.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": "No diff data found",
                    "error_category": "Diff Parsing Failure",
                    "severity": "warning"
                })
                continue
            author_name = row.get("Author Name", "Unknown")
            author_email = row.get("Author Email", "Unknown")
            for change in diff_data.get("diffs", []):
                source_info = change.get('source', {})
                dest_info = change.get('destination', {})
                file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                             else source_info.get('name', 'Unknown'))
                file_status = "modified"
                if not source_info:
                    file_status = "new"
                elif not dest_info:
                    file_status = "deleted"
                try:
                    added, removed, modified = parse_diff_change(change)
                    if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                        self.diff_failures.append({
                            "url": diff_url,
                            "commit_id": commit_id,
                            "reason": "No changes found in diff",
                            "error_category": "Diff Parsing Failure",
                            "severity": "warning"
                        })
                except Exception as e:
                    self.diff_failures.append({
                        "url": diff_url,
                        "commit_id": commit_id,
                        "reason": f"Exception during diff parsing: {e}",
                        "error_category": "Diff Parsing Failure",
                        "severity": "error"
                    })
                    continue
                out_row = {
                    "Project Key": project_key,
                    "Repo Slug": repo_slug,
                    "Author Name": author_name,
                    "Author Email": author_email,
                    "Month": commit_month,
                    "File Name": file_name,
                    "Month Lines Added": added,
                    "Month Lines Removed": removed,
                    "Month Lines Modified": modified,
                    "Total Commits": 1 if first_row else 0,
                    "Commit Id": commit_id,
                    "File Status": file_status
                }
                output_rows.append(out_row)
                first_row = False
            update_checkpoint(checkpoint_file, commit_id)
        return output_rows

    def write_chunk(self, rows: list, chunk_index: int, filename_prefix=None):
        if rows:
            df = pd.DataFrame(rows)
            if filename_prefix:
                output_path = os.path.join(self.output_folder, f"{filename_prefix}.parquet")
            else:
                output_path = os.path.join(self.output_folder, f"commit_file_output_part_{chunk_index}.parquet")
            df.to_parquet(output_path, engine="pyarrow", index=False)
            self.logger.info(f"Output written with {len(rows)} rows to {output_path}")
        else:
            self.logger.info(f"No rows to write for chunk {chunk_index}.")

    def write_failure_files(self):
        if self.failed_urls:
            df_failed = pd.DataFrame(self.failed_urls)
            failed_path = os.path.join(self.output_folder, "failed_urls.parquet")
            df_failed.to_parquet(failed_path, engine="pyarrow", index=False)
            self.logger.info(f"Failed URLs written to {failed_path}")
        if self.diff_failures:
            df_diff_fail = pd.DataFrame(self.diff_failures)
            diff_fail_path = os.path.join(self.output_folder, "diff_failures.parquet")
            df_diff_fail.to_parquet(diff_fail_path, engine="pyarrow", index=False)
            self.logger.info(f"Diff failures written to {diff_fail_path}")

# ------------------------------
# Main Processing Functions for SPK Mode (API-Only)
# ------------------------------
def process_spk_mode(input_folder: str, output_folder: str, start_date: str, end_date: str, recent_branches: int, order: str):
    project_keys_file = os.path.join(input_folder, 'WTMSPK.xlsx')
    filter_file = os.path.join(input_folder, 'author_filter.xlsx')
    wb = load_workbook(project_keys_file)
    sheet = wb.active
    spk_checkpoint_file = os.path.join(output_folder, "spk_checkpoint.txt")
    processed_spks = load_checkpoint(spk_checkpoint_file)
    
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    total_spks = len(spk_rows)
    if order == "bottom-up":
        spk_rows.reverse()
    spk_progress = 0

    filter_df = pd.read_excel(filter_file)
    author_email_filter = set(filter_df['Author Email'].str.lower())

    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    token_bucket = TokenBucket(capacity=70, tokens_per_second=1)
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    processor = CommitProcessor(api, output_folder, author_email_filter, recent_branches=1)

    successful_repos = 0
    total_repos = 0
    futures = {}
    with ThreadPoolExecutor(max_workers=5) as executor:
        for row in spk_rows:
            project_key = row[0]
            if not project_key or project_key in processed_spks:
                continue
            futures[project_key] = executor.submit(process_project, project_key, processor, start_date, end_date)
        for future in as_completed(futures):
            spk_progress += 1
            proj_key, repo_outputs = future.result()  # repo_outputs is list of (repo_slug, output_rows)
            for repo_slug, rows in repo_outputs:
                total_repos += 1
                if rows:
                    filename = f"output_{proj_key}_{repo_slug}"
                    processor.write_chunk(rows, 1, filename_prefix=filename)
                    main_logger.info(f"Output file for project {proj_key} repo {repo_slug}: {filename}.parquet")
                    successful_repos += 1
                else:
                    main_logger.info(f"No commits found for project {proj_key} repo {repo_slug}.")
            update_checkpoint(spk_checkpoint_file, proj_key)
            main_logger.info(f"Finished processing project {proj_key} ({spk_progress}/{total_spks})")
    processor.write_failure_files()
    main_logger.info(f"Successfully processed {successful_repos} out of {total_repos} repos.")
    main_logger.info(f"Total tokens consumed: {token_bucket.calls_made}")
    main_logger.info(f"Remaining token capacity: {token_bucket.tokens}")

def process_project(project_key: str, processor: CommitProcessor, start_date: str, end_date: str):
    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    api = processor.api
    repo_outputs = []  # List of tuples: (repo_slug, output_rows)
    repos_url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos?limit=1000"
    response, error_reason = api.make_request(repos_url)
    if not response:
        processor.failed_urls.append({
            "url": repos_url,
            "reason": error_reason,
            "error_category": "API Failure",
            "severity": "error"
        })
        return (project_key, repo_outputs)
    repos_data = response.json()
    for repo in repos_data.get('values', []):
        repo_slug = repo.get('slug')
        if not repo_slug:
            continue
        rows = processor.process_commits_spk(project_key, repo_slug, start_date, end_date)
        repo_outputs.append((repo_slug, rows))
    return (project_key, repo_outputs)

# ------------------------------
# Main Entry Point with Argument Parsing
# ------------------------------
def main():
    parser = argparse.ArgumentParser(description="Process Bitbucket commits (API-only, SPK mode).")
    parser.add_argument('--mode', choices=['spk', 'commit'], default='spk',
                        help="Input mode: 'spk' for SPK list mode, 'commit' for commit id mode.")
    parser.add_argument('--start_date', default='2024-01-01',
                        help="Start date (YYYY-MM-DD) for commit filtering (only used in SPK mode).")
    parser.add_argument('--end_date', default='2025-03-31',
                        help="End date (YYYY-MM-DD) for commit filtering (only used in SPK mode).")
    parser.add_argument('--recent_branches', type=int, default=1,
                        help="Number of recent branches to process (only used in SPK mode; fixed to 1).")
    parser.add_argument('--order', choices=['top-down', 'bottom-up'], default='top-down',
                        help="Processing order for SPKs (top-down or bottom-up).")
    parser.add_argument('--commit_chunk_size', type=int, default=1000,
                        help="Chunk size for processing commit mode (if used).")
    args = parser.parse_args()

    # Use a single common folder for both input and output
    common_folder = "C:/Riskportal/3Mar2025_JavaTrainings"
    input_folder = os.path.join(common_folder, "input")
    output_folder = os.path.join(common_folder, "output")
    
    # Reconfigure logging file handler to write into output folder:
    for handler in logging.getLogger().handlers[:]:
        logging.getLogger().removeHandler(handler)
    output_log_path = os.path.join(output_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(output_log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    global main_logger
    main_logger = logging.getLogger("Main")

    if args.mode == 'spk':
        main_logger.info("Running in SPK mode (API-only).")
        process_spk_mode(input_folder, output_folder, args.start_date, args.end_date, args.recent_branches, args.order)
    elif args.mode == 'commit':
        main_logger.info("Running in Commit Id mode.")
        process_commit_mode(input_folder, output_folder, args.commit_chunk_size)

if __name__ == "__main__":
    main()
###
