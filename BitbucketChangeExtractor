  import re
import threading
import logging
import json
import time
import os
import pandas as pd
import requests
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from openpyxl import load_workbook

# ------------------------------
# Custom JSON Logging Formatter
# ------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        # Include extra fields if they exist
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

# Set up the root logger with the JSON formatter
root_handler = logging.StreamHandler()
root_handler.setFormatter(JsonFormatter())
logging.basicConfig(level=logging.DEBUG, handlers=[root_handler])
main_logger = logging.getLogger("Main")

# ------------------------------
# Token Bucket for Rate Limiting
# ------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float):
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()

    def consume(self, tokens: int = 1):
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    return
            time.sleep(0.1)

# ------------------------------
# Bitbucket API Wrapper with Session Reuse
# ------------------------------
class BitbucketAPI:
    def __init__(self, base_url: str, username: str, app_password: str, token_bucket: TokenBucket):
        self.base_url = base_url
        self.username = username
        self.app_password = app_password
        self.token_bucket = token_bucket
        self.session = requests.Session()
        self.session.auth = (self.username, self.app_password)
        self.session.verify = False  # Ensure you trust the endpoint if disabling SSL verification
        self.logger = logging.getLogger("BitbucketAPI")

    def make_request(self, url: str, retries: int = 2, delay: int = 2):
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {url} (Attempt {attempt+1}/{retries})")
                response = self.session.get(url)
                response.raise_for_status()
                return response, None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                # Add structured logging fields
                extra = {"error_category": "API Failure", "severity": "error"}
                self.logger.error(f"Request failed for {url}: {error_message}", extra=extra)
                if response is not None and response.status_code in [404, 401]:
                    try:
                        error_message = response.json().get('errors', [{}])[0].get('message', error_message)
                        return None, error_message
                    except Exception:
                        pass
                if attempt < retries - 1:
                    time.sleep(delay)
                if response is not None and response.status_code == 429:
                    time.sleep(delay * 10)
                else:
                    return None, error_message
        return None, "Max retries exceeded"

# ------------------------------
# Helper Function: Diff Parsing for a Single Change
# ------------------------------
def parse_diff_change(change: dict) -> (int, int, int):
    logger_diff = logging.getLogger("DiffParser")
    lines_added = 0
    lines_removed = 0
    lines_modified = 0
    try:
        for hunk in change.get('hunks', []):
            for segment in hunk.get('segments', []):
                if segment.get('type') == 'REMOVED':
                    for _ in segment.get('lines', []):
                        lines_removed += 1
                elif segment.get('type') == 'ADDED':
                    for _ in segment.get('lines', []):
                        lines_added += 1
        # Modification detection logic could be added later
    except Exception as e:
        logger_diff.error(f"Error parsing diff change: {e}")
        raise e
    return lines_added, lines_removed, lines_modified

# ------------------------------
# Stub for Future Complexity and Commit Details Logic
# ------------------------------
def calculate_complexity_and_details(change: dict) -> (int, dict):
    # Isolated for future use
    return 0, {}

# ------------------------------
# Commit Processor: Handles Both Modes and Writes Output in Chunks
# ------------------------------
class CommitProcessor:
    def __init__(self, api: BitbucketAPI, output_folder: str, author_email_filter: set = None):
        self.api = api
        self.output_folder = output_folder
        self.author_email_filter = author_email_filter  # Only used in SPK mode
        self.logger = logging.getLogger("CommitProcessor")
        self.jira_pattern = r"\b[A-Z]+-\d+\b"
        self.failed_urls = []    # List of dicts for failed API calls
        self.diff_failures = []  # List of dicts for diff parsing failures

    def process_commits_spk(self, project_key: str, repo_slug: str, start_date: str, end_date: str):
        """Process commits for SPK mode (with filtering) and return output rows for this repo."""
        output_rows = []
        url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits?limit=1000"
        start_month = datetime.strptime(start_date, '%Y-%m-%d').strftime('%Y-%m')
        end_month = datetime.strptime(end_date, '%Y-%m-%d').strftime('%Y-%m')

        while True:
            response, error_reason = self.api.make_request(url)
            if not response:
                self.failed_urls.append({
                    "url": url, "reason": error_reason,
                    "error_category": "API Failure", "severity": "error"
                })
                break
            data = response.json()
            for commit in data.get('values', []):
                commit_date = datetime.fromtimestamp(commit["authorTimestamp"] / 1000)
                commit_month = commit_date.strftime('%Y-%m')
                if not (start_month <= commit_month <= end_month):
                    continue

                author_name = commit['author'].get('displayname', commit["author"].get('name', 'Unknown Author'))
                author_email = commit["author"].get('emailaddress', '').lower()
                if self.author_email_filter and author_email not in self.author_email_filter:
                    continue

                commit_id = commit.get('id', '')
                commit_message = commit.get('message', '')
                jira_ids = re.findall(self.jira_pattern, commit_message)
                is_merge_commit = len(commit.get('parents', [])) > 1

                self.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug}")
                diff_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
                file_response, diff_error = self.api.make_request(diff_url)
                if not file_response:
                    self.failed_urls.append({
                        "url": diff_url, "reason": diff_error,
                        "error_category": "API Failure", "severity": "error"
                    })
                    continue
                try:
                    diff_data = file_response.json()
                except Exception as e:
                    self.diff_failures.append({
                        "url": diff_url, "commit_id": commit_id, "reason": str(e),
                        "error_category": "Diff Parsing Failure", "severity": "error"
                    })
                    continue

                first_row = True
                if 'diffs' not in diff_data or not diff_data['diffs']:
                    self.diff_failures.append({
                        "url": diff_url, "commit_id": commit_id,
                        "reason": "No diff data found",
                        "error_category": "Diff Parsing Failure", "severity": "warning"
                    })
                    continue

                for change in diff_data.get("diffs", []):
                    source_info = change.get('source', {})
                    dest_info = change.get('destination', {})
                    file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                                 else source_info.get('name', 'Unknown'))
                    file_status = "modified"
                    if not source_info:
                        file_status = "new"
                    elif not dest_info:
                        file_status = "deleted"
                    try:
                        added, removed, modified = parse_diff_change(change)
                        if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                            self.diff_failures.append({
                                "url": diff_url, "commit_id": commit_id,
                                "reason": "No changes found in diff",
                                "error_category": "Diff Parsing Failure", "severity": "warning"
                            })
                    except Exception as e:
                        self.diff_failures.append({
                            "url": diff_url, "commit_id": commit_id,
                            "reason": f"Exception during diff parsing: {e}",
                            "error_category": "Diff Parsing Failure", "severity": "error"
                        })
                        continue

                    row = {
                        "Project Key": project_key,
                        "Repo Slug": repo_slug,
                        "Author Name": author_name,
                        "Author Email": author_email,
                        "Month": commit_month,
                        "File Name": file_name,
                        "Month Lines Added": added,
                        "Month Lines Removed": removed,
                        "Month Lines Modified": modified,
                        "Total Commits": 1 if first_row else 0,
                        "Commit Id": commit_id,
                        "File Status": file_status
                    }
                    output_rows.append(row)
                    first_row = False
            if data.get("isLastPage", True):
                break
            next_page = data.get('nextPageStart')
            url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits?limit=1000&start={next_page}"
        return output_rows

    def process_commits_by_ids(self, commit_id_file: str):
        """Process commits in commit mode from an input file with columns: Commit Id, Project Key, Repo Slug."""
        # Read the commit ids file
        df = pd.read_excel(commit_id_file)
        # Expecting columns: "Commit Id", "Project Key", "Repo Slug"
        output_rows = []
        for index, row in df.iterrows():
            commit_id = row["Commit Id"]
            project_key = row["Project Key"]
            repo_slug = row["Repo Slug"]
            # In commit mode, no author filtering or date range is applied.
            self.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug}")
            diff_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
            file_response, diff_error = self.api.make_request(diff_url)
            if not file_response:
                self.failed_urls.append({
                    "url": diff_url, "reason": diff_error,
                    "error_category": "API Failure", "severity": "error"
                })
                continue
            try:
                diff_data = file_response.json()
            except Exception as e:
                self.diff_failures.append({
                    "url": diff_url, "commit_id": commit_id, "reason": str(e),
                    "error_category": "Diff Parsing Failure", "severity": "error"
                })
                continue

            # Use commit's timestamp if available; otherwise, mark as Unknown month
            commit_month = "Unknown"
            if "authorTimestamp" in row:
                try:
                    commit_date = datetime.fromtimestamp(float(row["authorTimestamp"]) / 1000)
                    commit_month = commit_date.strftime('%Y-%m')
                except Exception:
                    commit_month = "Unknown"

            first_row = True
            if 'diffs' not in diff_data or not diff_data['diffs']:
                self.diff_failures.append({
                    "url": diff_url, "commit_id": commit_id,
                    "reason": "No diff data found",
                    "error_category": "Diff Parsing Failure", "severity": "warning"
                })
                continue

            # In commit mode, we don't have author filtering, so we'll set dummy values if needed.
            author_name = row.get("Author Name", "Unknown")
            author_email = row.get("Author Email", "Unknown")
            for change in diff_data.get("diffs", []):
                source_info = change.get('source', {})
                dest_info = change.get('destination', {})
                file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                             else source_info.get('name', 'Unknown'))
                file_status = "modified"
                if not source_info:
                    file_status = "new"
                elif not dest_info:
                    file_status = "deleted"
                try:
                    added, removed, modified = parse_diff_change(change)
                    if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                        self.diff_failures.append({
                            "url": diff_url, "commit_id": commit_id,
                            "reason": "No changes found in diff",
                            "error_category": "Diff Parsing Failure", "severity": "warning"
                        })
                except Exception as e:
                    self.diff_failures.append({
                        "url": diff_url, "commit_id": commit_id,
                        "reason": f"Exception during diff parsing: {e}",
                        "error_category": "Diff Parsing Failure", "severity": "error"
                    })
                    continue

                out_row = {
                    "Project Key": project_key,
                    "Repo Slug": repo_slug,
                    "Author Name": author_name,
                    "Author Email": author_email,
                    "Month": commit_month,
                    "File Name": file_name,
                    "Month Lines Added": added,
                    "Month Lines Removed": removed,
                    "Month Lines Modified": modified,
                    "Total Commits": 1 if first_row else 0,
                    "Commit Id": commit_id,
                    "File Status": file_status
                }
                output_rows.append(out_row)
                first_row = False
        return output_rows

    def write_chunk(self, rows: list, chunk_index: int):
        if rows:
            df = pd.DataFrame(rows)
            output_path = os.path.join(self.output_folder, f"commit_file_output_part_{chunk_index}.parquet")
            df.to_parquet(output_path, engine="pyarrow", index=False)
            self.logger.info(f"Chunk {chunk_index} written with {len(rows)} rows to {output_path}")
        else:
            self.logger.info(f"Chunk {chunk_index} has no rows; nothing to write.")

    def write_failure_files(self):
        # Write failed URLs
        if self.failed_urls:
            df_failed = pd.DataFrame(self.failed_urls)
            failed_path = os.path.join(self.output_folder, "failed_urls.parquet")
            df_failed.to_parquet(failed_path, engine="pyarrow", index=False)
            self.logger.info(f"Failed URLs written to {failed_path}")
        # Write diff failures
        if self.diff_failures:
            df_diff_fail = pd.DataFrame(self.diff_failures)
            diff_fail_path = os.path.join(self.output_folder, "diff_failures.parquet")
            df_diff_fail.to_parquet(diff_fail_path, engine="pyarrow", index=False)
            self.logger.info(f"Diff failures written to {diff_fail_path}")

# ------------------------------
# Main Processing Functions
# ------------------------------
def process_spk_mode(input_folder: str, output_folder: str, start_date: str, end_date: str):
    project_keys_file = os.path.join(input_folder, 'WTMSPK.xlsx')
    filter_file = os.path.join(input_folder, 'author_filter.xlsx')
    wb = load_workbook(project_keys_file)
    sheet = wb.active
    chunk_index = 1  # For chunked output

    # Load author filter (expects a column "Author Email")
    filter_df = pd.read_excel(filter_file)
    author_email_filter = set(filter_df['Author Email'].str.lower())

    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    token_bucket = TokenBucket(capacity=50, tokens_per_second=4)
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    processor = CommitProcessor(api, output_folder, author_email_filter)

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {}
        for row in sheet.iter_rows(min_row=2, values_only=True):
            project_key = row[0]
            if not project_key:
                continue
            main_logger.info(f"Processing project: {project_key}")
            repos_url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos?limit=1000"
            response, error_reason = api.make_request(repos_url)
            if not response:
                processor.failed_urls.append({
                    "url": repos_url, "reason": error_reason,
                    "error_category": "API Failure", "severity": "error"
                })
                continue
            repos_data = response.json()
            for repo in repos_data.get('values', []):
                repo_slug = repo.get('slug')
                if not repo_slug:
                    continue
                future = executor.submit(
                    processor.process_commits_spk,
                    project_key, repo_slug, start_date, end_date
                )
                futures[future] = (project_key, repo_slug)
        # Write chunk per repository as results come in
        for future in as_completed(futures):
            proj_key, repo_slug = futures[future]
            try:
                rows = future.result()
                # Write out the rows for this repository as a separate chunk
                processor.write_chunk(rows, chunk_index)
                chunk_index += 1
                main_logger.info(f"Finished processing repo {repo_slug} in project {proj_key}")
            except Exception as e:
                main_logger.error(f"Error processing repo {repo_slug} in project {proj_key}: {e}")
                processor.failed_urls.append({
                    "url": f"Repo: {repo_slug} in Project: {proj_key}",
                    "reason": str(e),
                    "error_category": "Processing Failure", "severity": "error"
                })
    processor.write_failure_files()

def process_commit_mode(input_folder: str, output_folder: str):
    # Input file containing commit IDs with columns: "Commit Id", "Project Key", "Repo Slug" (optionally Author details)
    commit_id_file = os.path.join(input_folder, 'commit_ids.xlsx')
    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    token_bucket = TokenBucket(capacity=50, tokens_per_second=4)
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    processor = CommitProcessor(api, output_folder)
    # For commit mode, process in one batch or use chunking if necessary.
    rows = processor.process_commits_by_ids(commit_id_file)
    # Write output as one chunk (or split into chunks if rows are huge)
    processor.write_chunk(rows, 1)
    processor.write_failure_files()

# ------------------------------
# Main Entry Point with Argument Parsing
# ------------------------------
def main():
    parser = argparse.ArgumentParser(description="Process Bitbucket commits.")
    parser.add_argument('--mode', choices=['spk', 'commit'], default='spk',
                        help="Input mode: 'spk' for SPK list mode, 'commit' for commit id mode.")
    parser.add_argument('--input_folder', default="C:/Riskportal/3Mar2025_JavaTrainings/input",
                        help="Folder containing input files.")
    parser.add_argument('--output_folder', default="C:/Riskportal/3Mar2025_JavaTrainings/output",
                        help="Folder to write output files.")
    parser.add_argument('--start_date', default='2024-01-01',
                        help="Start date (YYYY-MM-DD) for commit filtering (only used in SPK mode).")
    parser.add_argument('--end_date', default='2025-03-31',
                        help="End date (YYYY-MM-DD) for commit filtering (only used in SPK mode).")
    args = parser.parse_args()

    if args.mode == 'spk':
        main_logger.info("Running in SPK mode.")
        process_spk_mode(args.input_folder, args.output_folder, args.start_date, args.end_date)
    elif args.mode == 'commit':
        main_logger.info("Running in Commit Id mode.")
        process_commit_mode(args.input_folder, args.output_folder)

if __name__ == "__main__":
    main()
