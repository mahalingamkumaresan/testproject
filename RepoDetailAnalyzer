
import os
import time
import json
import random
import logging
import threading
import requests
import pandas as pd
from datetime import datetime
from openpyxl import load_workbook
from concurrent.futures import ThreadPoolExecutor, as_completed
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound

# Disable insecure request warnings
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ---------------------------------------------------------------------
# Configuration Variables
# ---------------------------------------------------------------------
USE_LEXER_ANALYSIS = True
BITBUCKET_BASE_URL = os.getenv(
    "BITBUCKET_BASE_URL",
    "https://scm.horizon.bankofamerica.com/rest/api/latest"
)
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")
INPUT_EXCEL = r"C:\riskportal\spk.xlsx"
OUTPUT_FOLDER = r"C:\riskportal\output"
TOKEN_BUCKET_CAPACITY = 75
TOKENS_PER_SECOND = 5
MAX_WORKERS = 5

# ---------------------------------------------------------------------
# Logging Configuration
# ---------------------------------------------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage(),
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

def configure_logging() -> logging.Logger:
    os.makedirs(os.path.join(OUTPUT_FOLDER, "logs"), exist_ok=True)
    log_path = os.path.join(OUTPUT_FOLDER, "logs", "process_log.txt")
    console = logging.StreamHandler()
    console.setFormatter(JsonFormatter())
    fileh = logging.FileHandler(log_path)
    fileh.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console, fileh])
    return logging.getLogger("BitbucketRepoAnalyzer")

logger = configure_logging()

# ---------------------------------------------------------------------
# Token Bucket Rate Limiter
# ---------------------------------------------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float):
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last = time.time()
        self.lock = threading.Lock()

    def consume(self, n: int = 1):
        while True:
            with self.lock:
                now = time.time()
                delta = now - self.last
                refill = delta * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last = now
                if self.tokens >= n:
                    self.tokens -= n
                    return
            time.sleep(0.1)

# ---------------------------------------------------------------------
# Generic API Client with Backoff
# ---------------------------------------------------------------------
class GenericAPIClient:
    def __init__(self, base_url: str, bucket: TokenBucket, auth: tuple = None):
        self.base_url = base_url.rstrip("/")
        self.bucket = bucket
        self.session = requests.Session()
        if auth:
            self.session.auth = auth
        self.session.verify = False
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.logger = logging.getLogger("GenericAPIClient")

    def make_request(self, endpoint: str, params: dict = None, retries: int = 3, base_delay: float = 1.0):
        url = f"{self.base_url}{endpoint}"
        for i in range(retries):
            self.bucket.consume(1)
            try:
                self.logger.debug(f"GET {url} params={params} attempt={i+1}")
                r = self.session.get(url, params=params)
                r.raise_for_status()
                return r.json(), None
            except requests.RequestException as e:
                code = r.status_code if 'r' in locals() else 'N/A'
                self.logger.error(f"{url} failed: {e} (status {code})",
                                  extra={"error_category":"API","severity":"error"})
                time.sleep(base_delay * (2**i) + random.random())
        return None, "Max retries exceeded"

# ---------------------------------------------------------------------
# Technology Mappings
# ---------------------------------------------------------------------
EXT_LANG_MAP = {
    ".java":"Java", ".kt":"Kotlin", ".sql":"SQL", ".pls":"PL/SQL",
    ".pks":"PL/SQL", ".pkb":"PL/SQL", ".py":"Python", ".js":"JavaScript",
    ".ts":"TypeScript", ".html":"HTML", ".htm":"HTML", ".sh":"Shell Script",
    ".bash":"Shell Script", ".ksh":"Shell Script", ".vbs":"VBScript",
    ".m":"Objective-C", ".swift":"Swift", ".cpp":"C++", ".c":"C",
    ".cs":"C#", ".jsp":"JSP", ".cbl":"COBOL", ".cob":"COBOL"
}
FILE_FRAMEWORK_MAP = {
    "pom.xml":"Maven", "build.gradle":"Gradle", "package.json":"Node.js",
    "requirements.txt":"Python (pip)", "Gemfile":"Ruby on Rails",
    "composer.json":"Composer (PHP)", "Cargo.toml":"Rust (Cargo)",
    "Makefile":"Make", "web.config":".NET Framework", "project.json":".NET Core",
    "struts.xml":"Struts", "applicationContext.xml":"Spring",
    "spring-servlet.xml":"Spring MVC", "hibernate.cfg.xml":"Hibernate",
    "beans.xml":"Java EE (CDI)", "mule-config.xml":"Mule ESB",
    ".twb":"Tableau", ".twbx":"Tableau", "angular.js":"AngularJS",
    "jquery.min.js":"jQuery"
}

# ---------------------------------------------------------------------
# Helpers: Branch, Commits, Files
# ---------------------------------------------------------------------
def get_top_branch(client, pk, slug):
    data, err = client.make_request(f"/projects/{pk}/repos/{slug}/branches",
                                     params={"orderBy":"MODIFICATION","limit":1})
    if err or not data.get("values"):
        logger.warning(f"{pk}/{slug}: falling back to master")
        return "master"
    return data["values"][0]["displayId"]

def get_last_commit_date(client, pk, slug, branch):
    data, err = client.make_request(f"/projects/{pk}/repos/{slug}/commits",
                                     params={"at":branch,"limit":1})
    if err or not data.get("values"):
        return "N/A"
    ts = data["values"][0].get("authorTimestamp",0)
    return datetime.fromtimestamp(ts/1000).strftime("%Y-%m-%d %H:%M:%S") if ts else "N/A"

def get_files_with_pagination(client, pk, slug, branch):
    files=[]; start=0; limit=1000
    while True:
        data, err = client.make_request(
            f"/projects/{pk}/repos/{slug}/files",
            params={"limit":limit,"start":start,"at":branch}
        )
        if err: break
        vals = data.get("values",[])
        files.extend(vals)
        nxt = data.get("nextPageStart")
        if not nxt: break
        start=nxt
    return files

def get_file_content(client, pk, slug, branch, path):
    p=path.lstrip("/")
    client.bucket.consume()
    try:
        r=client.session.get(
            f"{client.base_url}/projects/{pk}/repos/{slug}/raw/{p}",
            params={"at":branch}, verify=False
        )
        r.raise_for_status()
        return r.text
    except Exception as e:
        logger.error(f"raw/{p} error: {e}", extra={"error_category":"FileFetch"})
        return ""

# ---------------------------------------------------------------------
# Analyze file list for languages & frameworks
# ---------------------------------------------------------------------
def analyze_file_list(file_list, use_lexer, client, pk, slug, branch):
    uniq = {}
    for fp in file_list:
        ext = os.path.splitext(os.path.basename(fp))[1].lower()
        if ext not in uniq: uniq[ext]=fp

    langs, frs = set(), set()
    if use_lexer:
        for ext, sample in uniq.items():
            content = get_file_content(client, pk, slug, branch, sample)
            if content:
                try:
                    lx = guess_lexer_for_filename("x"+ext,content)
                    langs.add(lx.name)
                except ClassNotFound:
                    if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])
                except:
                    if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])
            else:
                if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])
    else:
        for ext in uniq:
            if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])

    for fp in file_list:
        bn=os.path.basename(fp)
        if bn in FILE_FRAMEWORK_MAP: frs.add(FILE_FRAMEWORK_MAP[bn])
        if "node_modules" in fp: frs.add("Node.js")
        if "venv" in fp or "env" in fp: frs.add("Python VirtualEnv")
        if "src/main/java" in fp: langs.add("Java")
        if "src/main/resources" in fp: frs.add("Spring MVC")

    exts = ["<no extension>" if e=="" else e for e in sorted(uniq)]
    return {
        "unique_extensions": exts,
        "languages": sorted(langs),
        "frameworks": sorted(frs),
        "file_count": len(file_list)
    }

# ---------------------------------------------------------------------
# SPK Output Management
# ---------------------------------------------------------------------
def spk_filepath(pk):
    return os.path.join(OUTPUT_FOLDER, f"{pk}_repos.xlsx")

def process_spk(client, pk):
    out = spk_filepath(pk)
    if os.path.isfile(out):
        logger.info(f"Skipping SPK {pk}: {out} exists")
        return
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    df = pd.DataFrame(columns=[
        "project_key","repo_slug","repo_id","repo_url","repo_name",
        "branch_name","last_commit_date","file_count",
        "unique_extensions","languages","frameworks"
    ])
    repos = get_repos(client, pk)
    for r in repos:
        slug=r["slug"]
        branch=get_top_branch(client,pk,slug)
        lcd=get_last_commit_date(client,pk,slug,branch)
        files=get_files_with_pagination(client,pk,slug,branch)
        an=analyze_file_list(files,USE_LEXER_ANALYSIS,client,pk,slug,branch)
        url=(r.get("links") or {}).get("self",[{}])[0].get("href",
            f"{client.base_url}/projects/{pk}/repos/{slug}")
        rec={
            "project_key":pk,"repo_slug":slug,"repo_id":r.get("id"),
            "repo_url":url,"repo_name":r.get("name",slug),
            "branch_name":branch,"last_commit_date":lcd,
            "file_count":an["file_count"],
            "unique_extensions":", ".join(an["unique_extensions"]),
            "languages":", ".join(an["languages"]),
            "frameworks":", ".join(an["frameworks"])
        }
        df = pd.concat([df, pd.DataFrame([rec])], ignore_index=True)
    df.to_excel(out, index=False)
    logger.info(f"Wrote SPK output: {out}")

def get_repos(client, pk):
    data, err = client.make_request(f"/projects/{pk}/repos", params={"limit":1000})
    if err: return []
    return data.get("values",[])

def main():
    start = datetime.now()
    logger.info("Starting BitbucketRepoAnalyzer")
    if not os.path.isfile(INPUT_EXCEL):
        logger.error(f"Missing input: {INPUT_EXCEL}")
        return
    wb=load_workbook(INPUT_EXCEL)
    spks=[r[0] for r in wb.active.iter_rows(min_row=2,values_only=True) if r and r[0]]
    bucket=TokenBucket(TOKEN_BUCKET_CAPACITY,TOKENS_PER_SECOND)
    client=GenericAPIClient(BITBUCKET_BASE_URL,bucket,auth=(BITBUCKET_USERNAME,BITBUCKET_APP_PASSWORD))
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        for pk in spks:
            ex.submit(process_spk, client, pk)
    logger.info(f"Completed in {datetime.now()-start}")

if __name__=="__main__":
    main()
