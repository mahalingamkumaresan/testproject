import pandas as pd
import requests
import orjson
from datetime import datetime
import logging
import time
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
from pathlib import Path
from typing import Tuple, Dict, Any
from logging.handlers import RotatingFileHandler
import threading

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

processing_folder = Path("C:/RiskPortal/13MarJavaTrainees")
input_folder = processing_folder / "input"
output_folder = processing_folder / "output"
output_folder.mkdir(parents=True, exist_ok=True)

BITBUCKET_BASE_URL = (
    os.getenv("BITBUCKET_BASE_URL", "https://scm.horizon.bankofamerica.com/rest/api/latest")
)
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")

# Flag to control output mode; here we write to separate files (batch processing)
WRITE_SAME_FILE = False  # For batch processing to separate files
use_pygments = True  # Default

# Batch size for processing (adjust as needed)
BATCH_SIZE = 1000

log_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
log_file = output_folder / "BB_data.log"
file_handler = RotatingFileHandler(log_file, maxBytes=5 * 1024 * 1024, backupCount=3)
file_handler.setFormatter(log_formatter)
file_handler.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
console_handler.setLevel(logging.INFO)
logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])

FILE_TYPE_CACHE: Dict[str, str] = {}

# Token Bucket implementation for rate limiting
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float):
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()

    def consume(self, tokens: int = 1):
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    return
            time.sleep(0.1)

token_bucket = TokenBucket(capacity=50, tokens_per_second=3)

# Basic authentication
session = requests.Session()
session.auth = (BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD)
session.verify = False

def identify_file_type(file_name: str, use_pygments: bool = False) -> str:
    try:
        if use_pygments:
            if file_name in FILE_TYPE_CACHE:
                return FILE_TYPE_CACHE[file_name]
            try:
                lexer = guess_lexer_for_filename(file_name, "")
                FILE_TYPE_CACHE[file_name] = lexer.name
                return lexer.name
            except ClassNotFound:
                return "MISC"
        return "NOT USED"
    except Exception as e:
        logging.error(f"identify_file_type error for {file_name}: {e}")
        return "MISC"

def make_request_with_retry(url: str, retries: int = 3, delay: int = 2) -> Tuple[Optional[requests.Response], Optional[str]]:
    for attempt in range(retries):
        token_bucket.consume(1)
        try:
            logging.debug(f"Requesting: {url} (Attempt {attempt+1}/{retries})")
            response = session.get(url)
            response.raise_for_status()
            return response, None
        except requests.exceptions.RequestException as e:
            error_message = str(e)
            if hasattr(e, "response") and e.response is not None:
                status = e.response.status_code
                if status == 401:
                    logging.error(f"Received 401 Unauthorized for URL {url}")
                if status == 429:
                    logging.error(f"Rate limit error on {url}: {error_message} (Attempt {attempt+1}/{retries})")
                    if attempt < retries - 1:
                        time.sleep(delay * (2 ** attempt))
                    else:
                        return None, error_message
                else:
                    logging.error(f"Error on {url}: {error_message} (Status: {status})")
                    return None, error_message
            else:
                logging.error(f"Error on {url}: {error_message} (No response object)")
                return None, error_message
    return None, "Unknown error"

def process_diff(diff: Dict[str, Any], file_name: str) -> Tuple[int, int, int]:
    try:
        source_info = diff.get("source") or {}
        destination_info = diff.get("destination") or {}
        file_name_val = source_info.get("name") or destination_info.get("name") or "UNKNOWN"
        truncated = source_info.get("truncated")
        if truncated is None:
            truncated = destination_info.get("truncated", False)
        if diff.get("binary", False) or truncated:
            return 0, 0, 0
        lines_added = 0
        lines_removed = 0
        lines_modified = 0
        removed_lines: Dict[Any, str] = {}
        added_lines: Dict[Any, str] = {}
        if file_name not in {file_name_val}:
            return 0, 0, 0
        for hunk in diff.get("hunks", []):
            for segment in hunk.get("segments", []):
                seg_type = segment.get("type")
                if seg_type == "REMOVED":
                    for line in segment.get("lines", []):
                        key = line.get("source")
                        removed_lines[key] = line.get("line", "").strip()
                        lines_removed += 1
                elif seg_type == "ADDED":
                    for line in segment.get("lines", []):
                        key = line.get("destination")
                        added_lines[key] = line.get("line", "").strip()
                        lines_added += 1
        for key, r_line in removed_lines.items():
            if key in added_lines and added_lines[key] != r_line:
                lines_modified += 1
                lines_added -= 1
                lines_removed -= 1
        return lines_added, lines_removed, lines_modified
    except Exception as e:
        logging.error(f"process_diff error for file {file_name}. Diff: {diff}. Error: {e}")
        return 0, 0, 0

def get_commits(df: pd.DataFrame, base_url: str, use_pygments: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if df.empty:
        logging.warning("Input file is empty!")
        return pd.DataFrame(), pd.DataFrame()
    failed_urls = []
    commit_details = []
    def process_commit(index: int, row: pd.Series) -> None:
        file_changes_url = "Not Available"
        try:
            commit_id = row["COMMIT_ID"]
            spk = row["SPK"]
            repository_name = row["REPOSITORY_NAME"]
            commit_type = str(row.get("COMMIT_TYPE", "")).strip().lower()
            full_name = row["FULL_NAME"]
            email_id = row["EMAIL_ID"]
            branch_name = row["BRANCH_NAME"]
            commit_date = pd.to_datetime(row["COMMIT_DATE"])
            commit_month_year = commit_date.strftime("%m-%Y")
            is_merge_commit = (commit_type == "merge")
            if is_merge_commit:
                file_changes_url = f"{base_url}/projects/{spk}/repos/{repository_name}/commits/{commit_id}"
                response, error_reason = make_request_with_retry(file_changes_url)
                if response:
                    try:
                        file_data = orjson.loads(response.text)
                    except Exception as e:
                        logging.error(f"JSON decode error for merge commit {commit_id} at {file_changes_url}: {e}")
                        failed_urls.append((file_changes_url, commit_id, f"JSON decode error: {e}"))
                        file_data = {}
                    committer_name = file_data.get("committer", {}).get("name")
                    committer_email = file_data.get("committer", {}).get("emailAddress")
                    if not committer_name or not committer_email:
                        committer_name = full_name
                        committer_email = email_id
                    effective_author_name = committer_name
                    effective_author_email = committer_email
                else:
                    error_msg = f"{error_reason}. Full URL: {file_changes_url}"
                    failed_urls.append((file_changes_url, commit_id, error_msg))
                    effective_author_name = full_name
                    effective_author_email = email_id
                commit_details.append({
                    "COMMIT_ID": commit_id,
                    "SPK": spk,
                    "REPOSITORY_NAME": repository_name,
                    "AUTHOR_NAME": effective_author_name,
                    "AUTHOR_EMAIL": effective_author_email,
                    "COMMITTER_NAME": committer_name if response else "",
                    "COMMITTER_EMAIL": committer_email if response else "",
                    "COMMIT_MONTH_YEAR": commit_month_year,
                    "BRANCH_NAME": branch_name,
                    "IS_MERGE_COMMIT": True,
                    "COMMIT_TYPE": commit_type,
                    "FILE_NAME": "Merge Commit",
                    "FILE_TYPE": "N/A",
                    "LINES_ADDED": 0,
                    "LINES_REMOVED": 0,
                    "LINES_MODIFIED": 0,
                    "DIFF_URL": file_changes_url
                })
            else:
                file_changes_url = f"{base_url}/projects/{spk}/repos/{repository_name}/commits/{commit_id}/diff?ignore_whitespace=true"
                response, error_reason = make_request_with_retry(file_changes_url)
                if response:
                    try:
                        file_data = orjson.loads(response.text)
                    except Exception as e:
                        logging.error(f"JSON decode error for commit {commit_id} at {file_changes_url}: {e}")
                        failed_urls.append((file_changes_url, commit_id, f"JSON decode error: {e}"))
                        return
                    diffs = file_data.get("diffs")
                    if not diffs:
                        reason = "No difference found or inaccessible commit"
                        failed_urls.append((file_changes_url, commit_id, reason))
                        return
                    for diff in diffs:
                        source_info = diff.get("source") or {}
                        destination_info = diff.get("destination") or {}
                        file_name_val = source_info.get("name") or destination_info.get("name") or "UNKNOWN"
                        file_type = identify_file_type(file_name_val, use_pygments)
                        try:
                            la, lr, lm = process_diff(diff, file_name_val)
                        except Exception as e:
                            logging.error(f"Diff processing error for commit {commit_id} for file {file_name_val} at {file_changes_url}: {e}")
                            continue
                        commit_details.append({
                            "COMMIT_ID": commit_id,
                            "SPK": spk,
                            "REPOSITORY_NAME": repository_name,
                            "AUTHOR_NAME": full_name,
                            "AUTHOR_EMAIL": email_id,
                            "COMMITTER_NAME": "",
                            "COMMITTER_EMAIL": "",
                            "COMMIT_MONTH_YEAR": commit_month_year,
                            "BRANCH_NAME": branch_name,
                            "IS_MERGE_COMMIT": False,
                            "COMMIT_TYPE": commit_type,
                            "FILE_NAME": file_name_val,
                            "FILE_TYPE": file_type,
                            "LINES_ADDED": la,
                            "LINES_REMOVED": lr,
                            "LINES_MODIFIED": lm,
                            "DIFF_URL": file_changes_url
                        })
                else:
                    error_msg = f"{error_reason}. Full URL: {file_changes_url}"
                    failed_urls.append((file_changes_url, commit_id, error_msg))
        except Exception as e:
            logging.error(f"Error processing commit at index {index} (COMMIT_ID: {row.get('COMMIT_ID', 'UNKNOWN')}, URL: {file_changes_url}): {e}")
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_commit, idx, row) for idx, row in df.iterrows()]
        for i, _ in enumerate(as_completed(futures), start=1):
            logging.info(f"Processed commit {i} of {len(df)}")
    commit_df = pd.DataFrame(commit_details)
    failed_df = pd.DataFrame(failed_urls, columns=["Failed URL", "CommitID", "Failure Reason"]) if failed_urls else pd.DataFrame()
    return commit_df, failed_df

def group_commit_data(commit_df: pd.DataFrame) -> pd.DataFrame:
    try:
        if commit_df.empty:
            return pd.DataFrame()
        grouped_df = commit_df.groupby([
            "SPK", "REPOSITORY_NAME", "AUTHOR_NAME", "AUTHOR_EMAIL", "COMMITTER_NAME",
            "COMMITTER_EMAIL", "COMMIT_MONTH_YEAR", "BRANCH_NAME", "IS_MERGE_COMMIT",
            "FILE_NAME", "FILE_TYPE"
        ]).agg({
            "LINES_ADDED": "sum",
            "LINES_REMOVED": "sum",
            "LINES_MODIFIED": "sum",
            "COMMIT_ID": "count",
            "DIFF_URL": lambda x: "; ".join(x.astype(str).unique())
        }).reset_index()
        grouped_df.rename(columns={"COMMIT_ID": "GROUP_COMMITS"}, inplace=True)
        total_df = commit_df.groupby(["AUTHOR_NAME", "COMMIT_MONTH_YEAR", "FILE_NAME"])["COMMIT_ID"]\
                            .count().reset_index(name="TOTAL_COMMITS")
        merged_df = pd.merge(grouped_df, total_df, on=["AUTHOR_NAME", "COMMIT_MONTH_YEAR", "FILE_NAME"], how="left")
        return merged_df
    except Exception as e:
        logging.error(f"group_commit_data error: {e}")
        return pd.DataFrame()

def aggregate_by_commit_id(commit_df: pd.DataFrame) -> pd.DataFrame:
    if commit_df.empty:
        return commit_df
    agg_funcs = {
        "SPK": "first",
        "REPOSITORY_NAME": "first",
        "AUTHOR_NAME": "first",
        "AUTHOR_EMAIL": "first",
        "COMMITTER_NAME": "first",
        "COMMITTER_EMAIL": "first",
        "COMMIT_MONTH_YEAR": "first",
        "BRANCH_NAME": "first",
        "IS_MERGE_COMMIT": "first",
        "COMMIT_TYPE": "first",
        "FILE_NAME": lambda x: "; ".join(x.astype(str).unique()),
        "FILE_TYPE": lambda x: "; ".join(x.astype(str).unique()),
        "LINES_ADDED": "sum",
        "LINES_REMOVED": "sum",
        "LINES_MODIFIED": "sum",
        "DIFF_URL": lambda x: "; ".join(x.astype(str).unique())
    }
    aggregated = commit_df.groupby("COMMIT_ID").agg(agg_funcs).reset_index()
    return aggregated

def generate_language_mapping(commit_df: pd.DataFrame) -> pd.DataFrame:
    if commit_df.empty:
        return pd.DataFrame()
    mapping = commit_df.copy()
    mapping["EXTENSION"] = mapping["FILE_NAME"].apply(lambda x: x.split('.')[-1].lower() if '.' in x else "no_ext")
    mapping = mapping[["EXTENSION", "FILE_TYPE"]].drop_duplicates().reset_index(drop=True)
    return mapping

def main() -> None:
    try:
        input_file = input_folder / "CommitInputs.xlsx"
        df = pd.read_excel(input_file)
        df.columns = [col.strip().upper() for col in df.columns]
        logging.info(f"Processing {len(df)} commits from input file.")

        # Process in batches to avoid high memory usage
        total = len(df)
        batch_outputs = []
        batch_failed = []
        for start in range(0, total, BATCH_SIZE):
            batch_df = df.iloc[start: start+BATCH_SIZE]
            logging.info(f"Processing batch {start//BATCH_SIZE + 1} with {len(batch_df)} commits")
            commit_batch, failed_batch = get_commits(batch_df, BITBUCKET_BASE_URL, use_pygments)
            # Write each batch's commit details to a separate Parquet file
            batch_output_file = output_folder / f"extended_data_batch_{start//BATCH_SIZE + 1}.parquet"
            commit_batch.to_parquet(batch_output_file, engine="pyarrow", index=False)
            if not failed_batch.empty:
                batch_failed_file = output_folder / f"failed_urls_batch_{start//BATCH_SIZE + 1}.parquet"
                failed_batch.to_parquet(batch_failed_file, engine="pyarrow", index=False)
            batch_outputs.append(commit_batch)
            batch_failed.append(failed_batch)
        # Optionally, combine all batch commit outputs for language mapping
        if batch_outputs:
            combined_commits = pd.concat(batch_outputs, ignore_index=True)
        else:
            combined_commits = pd.DataFrame()
        lang_map_df = generate_language_mapping(combined_commits)
        lang_map_file = output_folder / "language_mapping.parquet"
        lang_map_df.to_parquet(lang_map_file, engine="pyarrow", index=False)

        # Optionally, combine failed batches and write one file
        if batch_failed:
            combined_failed = pd.concat(batch_failed, ignore_index=True)
            if not combined_failed.empty:
                failed_file = output_folder / "failed_urls.parquet"
                combined_failed.to_parquet(failed_file, engine="pyarrow", index=False)

        logging.info("Batch processing completed")
    except Exception as e:
        logging.error(f"Unexpected error in main: {e}", exc_info=True)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Script terminated due to unexpected error: {e}", exc_info=True)
