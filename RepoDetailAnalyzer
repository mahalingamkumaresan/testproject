

import os
import time
import json
import random
import logging
import threading
import requests
import pandas as pd
from datetime import datetime
from openpyxl import load_workbook
from concurrent.futures import ThreadPoolExecutor, as_completed
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound

# ---------------------------------------------------------------------
# Configuration Variables
# ---------------------------------------------------------------------
USE_LEXER_ANALYSIS = True   # Use real lexer analysis to guess language from file content.
BITBUCKET_BASE_URL = os.getenv("BITBUCKET_BASE_URL", "https://scm.example.com/rest/api/1.0")
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")
INPUT_EXCEL = r"C:\riskportal\WTMSPK.xlsx"
OUTPUT_EXCEL = r"C:\riskportal\spk_repo_details.xlsx"
TOKEN_BUCKET_CAPACITY = 75
TOKENS_PER_SECOND = 5
MAX_WORKERS = 5
MAX_REPOS_PER_SPK = 10  # Set to None to process all repos.

# ---------------------------------------------------------------------
# Logging Configuration
# ---------------------------------------------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

def configure_logging() -> logging.Logger:
    logs_folder = r"C:\riskportal\logs"
    os.makedirs(logs_folder, exist_ok=True)
    log_path = os.path.join(logs_folder, "process_log.txt")

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())

    file_handler = logging.FileHandler(log_path)
    file_handler.setFormatter(JsonFormatter())

    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    return logging.getLogger("BitbucketRepoAnalyzer")

logger = configure_logging()

# ---------------------------------------------------------------------
# Rate Limiting with Token Bucket
# ---------------------------------------------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float) -> None:
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1) -> None:
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ---------------------------------------------------------------------
# Generic API Client with Exponential Backoff
# ---------------------------------------------------------------------
class GenericAPIClient:
    def __init__(self, base_url: str, token_bucket: TokenBucket, auth: tuple = None):
        self.base_url = base_url.rstrip("/")
        self.token_bucket = token_bucket
        self.auth = auth
        self.session = requests.Session()
        if self.auth:
            self.session.auth = self.auth
        # For simplicity, disable SSL verification (adjust as needed)
        self.session.verify = False
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.logger = logging.getLogger("GenericAPIClient")

    def make_request(self, endpoint: str, params: dict = None, retries: int = 3, base_delay: float = 1.0):
        full_url = f"{self.base_url}{endpoint}"
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {full_url} (Attempt {attempt+1}/{retries}), params={params}")
                response = self.session.get(full_url, params=params)
                response.raise_for_status()
                return response.json(), None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                status_code = response.status_code if response else "N/A"
                self.logger.error(
                    f"Error calling {full_url}: {error_message} (Status: {status_code})",
                    extra={"error_category": "API Failure", "severity": "error"}
                )
                sleep_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                time.sleep(sleep_time)
        return None, "Max retries exceeded"

# ---------------------------------------------------------------------
# Function to Get Raw File Content from Bitbucket
# ---------------------------------------------------------------------
def get_file_content(api_client: GenericAPIClient, project_key: str, repo_slug: str, branch: str, file_path: str) -> str:
    """
    Fetches the raw content of a file from Bitbucket.
    Endpoint: /projects/{project_key}/repos/{repo_slug}/raw/{file_path}?at={branch}
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/raw/{file_path}"
    params = {"at": branch}
    # Consume token for raw file request.
    api_client.token_bucket.consume(1)
    try:
        url = f"{api_client.base_url}{endpoint}"
        response = api_client.session.get(url, params=params)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logger.error(f"Failed to fetch content for {file_path} in {project_key}/{repo_slug}: {e}",
                     extra={"error_category": "File Fetch Error", "severity": "error"})
        return ""

# ---------------------------------------------------------------------
# Mappings for Extension-Based Analysis (Fallback)
# ---------------------------------------------------------------------
EXT_LANG_MAP = {
    ".py": "Python",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
    ".rb": "Ruby",
    ".cs": "C#",
    ".cpp": "C++",
    ".c": "C",
    ".go": "Go",
    ".php": "PHP",
    ".rs": "Rust",
    ".kt": "Kotlin",
    ".swift": "Swift",
    # Extend this map if needed.
}

FILE_FRAMEWORK_MAP = {
    "pom.xml": "Maven",
    "build.gradle": "Gradle",
    "package.json": "Node.js / npm",
    "requirements.txt": "Python (pip)",
    "Gemfile": "Ruby on Rails",
    "composer.json": "Composer (PHP)",
    "Cargo.toml": "Rust (Cargo)",
    "Makefile": "Make",
    # Extend this map if needed.
}

# ---------------------------------------------------------------------
# API Calls: Retrieve Repositories & Files
# ---------------------------------------------------------------------
def get_repos(api_client: GenericAPIClient, project_key: str) -> list:
    """Retrieve all repositories for the given SPK."""
    endpoint = f"/projects/{project_key}/repos"
    params = {"limit": 1000}
    data, error = api_client.make_request(endpoint, params=params)
    if error:
        logger.error(
            f"Failed to get repos for project {project_key}: {error}",
            extra={"error_category": "API Failure", "severity": "error"}
        )
        return []
    return data.get("values", [])

def get_files_with_pagination(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> list:
    """
    Retrieve all files in the repository using a single or multiple calls (limit=1000).
    Pagination stops when there is no nextPageStart or it is None.
    """
    all_files = []
    start = 0
    limit = 1000
    while True:
        endpoint = f"/projects/{project_key}/repos/{repo_slug}/files"
        params = {"limit": limit, "start": start}
        response_data, error = api_client.make_request(endpoint, params=params)
        if error:
            logger.error(
                f"Failed to retrieve files for {project_key}/{repo_slug} (start={start}): {error}",
                extra={"error_category": "API Failure", "severity": "error"}
            )
            break
        file_values = response_data.get("values", [])
        all_files.extend(file_values)
        # Check for nextPageStart; if missing or None, stop.
        next_page = response_data.get("nextPageStart")
        if not next_page:
            break
        start = next_page
    return all_files

# ---------------------------------------------------------------------
# Analysis Logic: Using Lexer if Enabled, Else Fallback
# ---------------------------------------------------------------------
def analyze_file_list(file_list: list, use_lexer: bool, api_client=None, project_key=None, repo_slug=None, branch="master") -> dict:
    """
    If use_lexer is True, fetch a sample file for each unique extension and use Pygments to guess the language.
    Otherwise, use extension-based mapping.
    In both cases, also apply folder heuristics and filename mapping for frameworks.
    """
    unique_extensions = {}  # mapping extension -> sample file path
    for file_path in file_list:
        base_name = os.path.basename(file_path)
        ext = os.path.splitext(base_name)[1].lower()
        if ext and ext not in unique_extensions:
            unique_extensions[ext] = file_path

    languages = set()
    frameworks = set()

    if use_lexer and api_client and project_key and repo_slug:
        # For each unique extension, fetch sample file content and determine language
        for ext, sample_file in unique_extensions.items():
            content = get_file_content(api_client, project_key, repo_slug, branch, sample_file)
            if content:
                try:
                    # Use a dummy filename with the extension for better guessing.
                    lexer = guess_lexer_for_filename("dummy" + ext, content)
                    detected_language = lexer.name
                    languages.add(detected_language)
                except ClassNotFound:
                    # Fallback if lexer cannot be determined.
                    if ext in EXT_LANG_MAP:
                        languages.add(EXT_LANG_MAP[ext])
                except Exception as e:
                    logger.error(f"Lexer error for file {sample_file}: {e}",
                                 extra={"error_category": "Lexer Error", "severity": "error"})
                    if ext in EXT_LANG_MAP:
                        languages.add(EXT_LANG_MAP[ext])
            else:
                # If no content available, fallback to mapping.
                if ext in EXT_LANG_MAP:
                    languages.add(EXT_LANG_MAP[ext])
    else:
        # Fallback: use extension-based mapping.
        for ext in unique_extensions.keys():
            if ext in EXT_LANG_MAP:
                languages.add(EXT_LANG_MAP[ext])

    # Additionally, use folder heuristics and filename checks across all files.
    for file_path in file_list:
        base_name = os.path.basename(file_path)
        if base_name in FILE_FRAMEWORK_MAP:
            frameworks.add(FILE_FRAMEWORK_MAP[base_name])
        # Folder heuristics:
        if "node_modules" in file_path:
            frameworks.add("Node.js")
        if "venv" in file_path or "env" in file_path:
            frameworks.add("Python VirtualEnv")
        if "src/main/java" in file_path:
            languages.add("Java")
        if "src/main/resources" in file_path:
            frameworks.add("Spring")

    return {
        "unique_extensions": list(sorted(unique_extensions.keys())),
        "languages": list(sorted(languages)),
        "frameworks": list(sorted(frameworks)),
        "file_count": len(file_list)
    }

# ---------------------------------------------------------------------
# Excel Management: Read Existing Output and Append New Records
# ---------------------------------------------------------------------
def load_existing_excel(filepath: str) -> pd.DataFrame:
    """Load existing output Excel, or return an empty DataFrame with defined columns."""
    if not os.path.isfile(filepath):
        columns = [
            "project_key", "repo_slug", "repo_id", "repo_url", "repo_name",
            "file_count", "unique_extensions", "languages", "frameworks"
        ]
        return pd.DataFrame(columns=columns)
    return pd.read_excel(filepath)

def already_processed(spk: str, repo_slug: str, df_existing: pd.DataFrame) -> bool:
    """Check if the given (SPK, repo_slug) combination is already in the DataFrame."""
    mask = (df_existing["project_key"] == spk) & (df_existing["repo_slug"] == repo_slug)
    return mask.any()

def append_and_save(filepath: str, df_existing: pd.DataFrame, new_record: dict):
    """Append a new record and write the DataFrame back to Excel."""
    df_new = pd.DataFrame([new_record])
    df_concat = pd.concat([df_existing, df_new], ignore_index=True)
    df_concat.to_excel(filepath, index=False)

# ---------------------------------------------------------------------
# Process a Single Repository: Check & Process
# ---------------------------------------------------------------------
def process_single_repo(api_client: GenericAPIClient, df_existing: pd.DataFrame, project_key: str, repo: dict) -> pd.DataFrame:
    repo_slug = repo.get("slug")
    if already_processed(project_key, repo_slug, df_existing):
        logger.info(f"Skipping repo {repo_slug} in project {project_key} because it's already processed.")
        return df_existing

    # Determine default branch from repo details if available.
    default_branch = "master"
    if repo.get("defaultBranch") and isinstance(repo.get("defaultBranch"), dict):
        default_branch = repo.get("defaultBranch").get("displayId", "master")

    file_paths = get_files_with_pagination(api_client, project_key, repo_slug)
    analysis = analyze_file_list(file_paths, USE_LEXER_ANALYSIS, api_client, project_key, repo_slug, default_branch)

    repo_id = repo.get("id")
    repo_name = repo.get("name", repo_slug)
    links = repo.get("links", {})
    self_links = links.get("self", [])
    if self_links and isinstance(self_links, list):
        repo_url = self_links[0].get("href", f"{api_client.base_url}/projects/{project_key}/repos/{repo_slug}")
    else:
        repo_url = f"{api_client.base_url}/projects/{project_key}/repos/{repo_slug}"

    new_record = {
        "project_key": project_key,
        "repo_slug": repo_slug,
        "repo_id": repo_id,
        "repo_url": repo_url,
        "repo_name": repo_name,
        "file_count": analysis["file_count"],
        "unique_extensions": ", ".join(analysis["unique_extensions"]),
        "languages": ", ".join(analysis["languages"]),
        "frameworks": ", ".join(analysis["frameworks"])
    }
    logger.info(f"Processed {project_key}/{repo_slug} -> {new_record}")
    append_and_save(OUTPUT_EXCEL, df_existing, new_record)
    df_updated = load_existing_excel(OUTPUT_EXCEL)
    return df_updated

# ---------------------------------------------------------------------
# Process a Single SPK (Project)
# ---------------------------------------------------------------------
def process_single_spk(api_client: GenericAPIClient, project_key: str, max_repos: int, df_existing: pd.DataFrame) -> pd.DataFrame:
    repos = get_repos(api_client, project_key)
    if max_repos is not None:
        repos = repos[:max_repos]
    for repo in repos:
        df_existing = process_single_repo(api_client, df_existing, project_key, repo)
    return df_existing

# ---------------------------------------------------------------------
# Main Orchestration
# ---------------------------------------------------------------------
def main():
    start_time = datetime.now()
    logger.info("BitbucketRepoAnalyzer started.")

    # Load existing output data
    df_existing = load_existing_excel(OUTPUT_EXCEL)

    if not os.path.isfile(INPUT_EXCEL):
        logger.error(f"Input SPK file not found: {INPUT_EXCEL}")
        return

    wb = load_workbook(INPUT_EXCEL)
    sheet = wb.active
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    spk_list = [r[0] for r in spk_rows if r[0]]

    token_bucket = TokenBucket(TOKEN_BUCKET_CAPACITY, TOKENS_PER_SECOND)
    api_client = GenericAPIClient(BITBUCKET_BASE_URL, token_bucket, auth=(BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD))

    # Process each SPK in parallel.
    def spk_worker(spk_key: str) -> str:
        nonlocal df_existing
        df_existing = process_single_spk(api_client, spk_key, MAX_REPOS_PER_SPK, df_existing)
        return spk_key

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(spk_worker, spk): spk for spk in spk_list}
        for future in as_completed(futures):
            spk_key = futures[future]
            future.result()  # This will raise any exceptions
            logger.info(f"Finished processing SPK {spk_key}")

    end_time = datetime.now()
    logger.info(f"BitbucketRepoAnalyzer completed in {end_time - start_time}")

if __name__ == "__main__":
    main()
