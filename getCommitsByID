#!/usr/bin/env python
import pandas as pd
import requests
import orjson
from openpyxl import load_workbook, Workbook
from datetime import datetime
import logging
import os
import time
import json
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
from logging.handlers import RotatingFileHandler

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

processing_folder = Path("C:/RiskPortal/13MarJavaTrainees")
input_folder = processing_folder / "input"
output_folder = processing_folder / "output"
output_folder.mkdir(parents=True, exist_ok=True)

BITBUCKET_BASE_URL = os.getenv("BITBUCKET_BASE_URL", "https://scm.horizon.bankofamerica.com/rest/api/latest")
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")

# Flag: if True, extended data is merged into the input Excel file; if False, output is written to a separate file.
WRITE_SAME_FILE = True

# Set use_pygments to True by default.
use_pygments = True

log_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
log_file = output_folder / "BB_data.log"
file_handler = RotatingFileHandler(log_file, maxBytes=5 * 1024 * 1024, backupCount=3)
file_handler.setFormatter(log_formatter)
file_handler.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
console_handler.setLevel(logging.INFO)
logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])

FILE_TYPE_CACHE: Dict[str, str] = {}

# Use Basic Authentication.
session = requests.Session()
session.auth = (BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD)
session.verify = False

def identify_file_type(file_name: str, use_pygments: bool = False) -> str:
    try:
        if use_pygments:
            if file_name in FILE_TYPE_CACHE:
                return FILE_TYPE_CACHE[file_name]
            try:
                lexer = guess_lexer_for_filename(file_name, "")
                FILE_TYPE_CACHE[file_name] = lexer.name
                return lexer.name
            except ClassNotFound:
                return "UNKNOWN"
        return "NOT USED"
    except Exception as e:
        logging.error(f"Error in identify_file_type for {file_name}: {e}")
        return "UNKNOWN"

def make_request_with_retry(url: str, retries: int = 3, delay: int = 2) -> Tuple[Optional[requests.Response], Optional[str]]:
    for attempt in range(retries):
        try:
            logging.debug(f"Requesting: {url} (Attempt {attempt+1}/{retries})")
            response = session.get(url)
            response.raise_for_status()
            return response, None
        except requests.exceptions.RequestException as e:
            error_message = str(e)
            if hasattr(e, "response") and e.response is not None:
                status = e.response.status_code
                if status == 401:
                    logging.error(f"Received 401 Unauthorized for URL {url}")
                if status == 429:
                    logging.error(f"Rate limit error on {url}: {error_message} (Attempt {attempt+1}/{retries})")
                    if attempt < retries - 1:
                        time.sleep(delay * (2 ** attempt))
                    else:
                        return None, error_message
                else:
                    logging.error(f"Error on {url}: {error_message} (Status code: {status}, not retrying)")
                    return None, error_message
            else:
                logging.error(f"Error on {url}: {error_message} (No response object)")
                return None, error_message
    return None, "Unknown error"

def process_diff(diff: Dict[str, Any], file_name: str) -> Tuple[int, int, int]:
    try:
        if diff.get("binary", False) or diff.get("source", {}).get("truncated", False):
            return 0, 0, 0
        lines_added = 0
        lines_removed = 0
        lines_modified = 0
        removed_lines: Dict[Any, str] = {}
        added_lines: Dict[Any, str] = {}
        dest_name = diff.get("destination", {}).get("name")
        source_name = diff.get("source", {}).get("name") if diff.get("source") else None
        if file_name not in {dest_name, source_name}:
            return 0, 0, 0
        for hunk in diff.get("hunks", []):
            for segment in hunk.get("segments", []):
                seg_type = segment.get("type")
                if seg_type == "REMOVED":
                    for line in segment.get("lines", []):
                        line_number = line.get("source")
                        removed_lines[line_number] = line.get("line", "").strip()
                        lines_removed += 1
                elif seg_type == "ADDED":
                    for line in segment.get("lines", []):
                        line_number = line.get("destination")
                        added_lines[line_number] = line.get("line", "").strip()
                        lines_added += 1
        for key, removed_line in removed_lines.items():
            if key in added_lines and added_lines[key] != removed_line:
                lines_modified += 1
                lines_added -= 1
                lines_removed -= 1
        return lines_added, lines_removed, lines_modified
    except Exception as e:
        logging.error(f"Error in process_diff for commit (URL unknown) for file {file_name}. Diff: {diff}. Error: {e}")
        return 0, 0, 0

def get_commits(df: pd.DataFrame, base_url: str, use_pygments: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if df.empty:
        logging.warning("Input file is empty! Exiting.")
        return pd.DataFrame(), pd.DataFrame()
    failed_urls = []
    commit_details = []
    def process_commit(index: int, row: pd.Series) -> None:
        file_changes_url = "Not Available"
        try:
            commit_id = row["COMMIT_ID"]
            spk = row["SPK"]
            repository_name = row["REPOSITORY_NAME"]
            commit_type = str(row.get("COMMIT_TYPE", "")).strip().lower()
            full_name = row["FULL_NAME"]
            email_id = row["EMAIL_ID"]
            branch_name = row["BRANCH_NAME"]
            commit_date = pd.to_datetime(row["COMMIT_DATE"])
            commit_month_year = commit_date.strftime("%m-%Y")
            is_merge_commit = (commit_type == "merge")
            file_changes_url = f"{base_url}/projects/{spk}/repos/{repository_name}/commits/{commit_id}/diff?ignore_whitespace=true"
            response, error_reason = make_request_with_retry(file_changes_url)
            if is_merge_commit:
                if response:
                    try:
                        file_data = orjson.loads(response.text)
                    except Exception as e:
                        logging.error(f"Error decoding JSON for merge commit {commit_id} at URL {file_changes_url}: {e}")
                        failed_urls.append((file_changes_url, commit_id, f"{str(e)}"))
                        file_data = {}
                    committer_name = file_data.get("committer", {}).get("name")
                    committer_email = file_data.get("committer", {}).get("emailAddress")
                    if not committer_name or not committer_email:
                        committer_name = full_name
                        committer_email = email_id
                    effective_author_name = committer_name
                    effective_author_email = committer_email
                else:
                    error_msg = f"{error_reason}. Full URL: {file_changes_url}"
                    failed_urls.append((file_changes_url, commit_id, error_msg))
                    effective_author_name = full_name
                    effective_author_email = email_id

                commit_details.append({
                    "COMMIT_ID": commit_id,
                    "SPK": spk,
                    "REPOSITORY_NAME": repository_name,
                    "AUTHOR_NAME": effective_author_name,
                    "AUTHOR_EMAIL": effective_author_email,
                    "COMMITTER_NAME": committer_name if response else "",
                    "COMMITTER_EMAIL": committer_email if response else "",
                    "COMMIT_MONTH_YEAR": commit_month_year,
                    "BRANCH_NAME": branch_name,
                    "IS_MERGE_COMMIT": is_merge_commit,
                    "COMMIT_TYPE": commit_type,
                    "FILE_NAME": "Merge Commit",
                    "FILE_TYPE": "N/A",
                    "LINES_ADDED": 0,
                    "LINES_REMOVED": 0,
                    "LINES_MODIFIED": 0,
                    "DIFF_URL": file_changes_url
                })
            else:
                if response:
                    try:
                        file_data = orjson.loads(response.text)
                    except Exception as e:
                        logging.error(f"Error decoding JSON for commit {commit_id} at URL {file_changes_url}: {e}")
                        failed_urls.append((file_changes_url, commit_id, f"{str(e)}"))
                        return
                    for diff in file_data.get("diffs", []):
                        file_name = diff.get("source", {}).get("name", "UNKNOWN")
                        file_type = identify_file_type(file_name, use_pygments)
                        try:
                            la, lr, lm = process_diff(diff, file_name)
                        except Exception as e:
                            logging.error(f"Error processing diff for commit {commit_id} for file {file_name} at URL {file_changes_url}: {e}")
                            continue
                        commit_details.append({
                            "COMMIT_ID": commit_id,
                            "SPK": spk,
                            "REPOSITORY_NAME": repository_name,
                            "AUTHOR_NAME": full_name,
                            "AUTHOR_EMAIL": email_id,
                            "COMMITTER_NAME": "",
                            "COMMITTER_EMAIL": "",
                            "COMMIT_MONTH_YEAR": commit_month_year,
                            "BRANCH_NAME": branch_name,
                            "IS_MERGE_COMMIT": is_merge_commit,
                            "COMMIT_TYPE": commit_type,
                            "FILE_NAME": file_name,
                            "FILE_TYPE": file_type,
                            "LINES_ADDED": la,
                            "LINES_REMOVED": lr,
                            "LINES_MODIFIED": lm,
                            "DIFF_URL": file_changes_url
                        })
                else:
                    error_msg = f"{error_reason}. Full URL: {file_changes_url}"
                    failed_urls.append((file_changes_url, commit_id, error_msg))
        except Exception as e:
            logging.error(f"Error processing commit at index {index} (COMMIT_ID: {row.get('COMMIT_ID', 'UNKNOWN')}, URL: {file_changes_url}): {e}")

    total_commits = len(df)
    processed_count = 0
    try:
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(process_commit, idx, row) for idx, row in df.iterrows()]
            for future in as_completed(futures):
                processed_count += 1
                logging.info(f"Processed commit {processed_count} of {total_commits}")
    except Exception as e:
        logging.error(f"Error during concurrent commit processing: {e}")

    commit_df = pd.DataFrame(commit_details)
    failed_df = pd.DataFrame(failed_urls, columns=["Failed URL", "CommitID", "Failure Reason"]) if failed_urls else pd.DataFrame()
    return commit_df, failed_df

def group_commit_data(commit_df: pd.DataFrame) -> pd.DataFrame:
    try:
        if commit_df.empty:
            return pd.DataFrame()
        grouped_df = commit_df.groupby([
            "SPK", "REPOSITORY_NAME", "AUTHOR_NAME", "AUTHOR_EMAIL", "COMMITTER_NAME",
            "COMMITTER_EMAIL", "COMMIT_MONTH_YEAR", "BRANCH_NAME", "IS_MERGE_COMMIT",
            "FILE_NAME", "FILE_TYPE"
        ]).agg({
            "LINES_ADDED": "sum",
            "LINES_REMOVED": "sum",
            "LINES_MODIFIED": "sum",
            "COMMIT_ID": "count",
            "DIFF_URL": lambda x: "; ".join(x.astype(str).unique())
        }).reset_index()
        grouped_df.rename(columns={"COMMIT_ID": "GROUP_COMMITS"}, inplace=True)
        total_df = commit_df.groupby(["AUTHOR_NAME", "COMMIT_MONTH_YEAR", "FILE_NAME"])["COMMIT_ID"]\
                            .count().reset_index(name="TOTAL_COMMITS")
        merged_df = pd.merge(grouped_df, total_df, on=["AUTHOR_NAME", "COMMIT_MONTH_YEAR", "FILE_NAME"], how="left")
        return merged_df
    except Exception as e:
        logging.error(f"Error in group_commit_data: {e}")
        return pd.DataFrame()

def aggregate_by_commit_id(commit_df: pd.DataFrame) -> pd.DataFrame:
    if commit_df.empty:
        return commit_df
    agg_funcs = {
        "SPK": "first",
        "REPOSITORY_NAME": "first",
        "AUTHOR_NAME": "first",
        "AUTHOR_EMAIL": "first",
        "COMMITTER_NAME": "first",
        "COMMITTER_EMAIL": "first",
        "COMMIT_MONTH_YEAR": "first",
        "BRANCH_NAME": "first",
        "IS_MERGE_COMMIT": "first",
        "COMMIT_TYPE": "first",
        "FILE_NAME": lambda x: "; ".join(x.astype(str).unique()),
        "FILE_TYPE": lambda x: "; ".join(x.astype(str).unique()),
        "LINES_ADDED": "sum",
        "LINES_REMOVED": "sum",
        "LINES_MODIFIED": "sum",
        "DIFF_URL": lambda x: "; ".join(x.astype(str).unique())
    }
    aggregated = commit_df.groupby("COMMIT_ID").agg(agg_funcs).reset_index()
    return aggregated

def main() -> None:
    try:
        input_file = input_folder / "CommitInputs.xlsx"
        df = pd.read_excel(input_file)
        df.columns = [col.strip().upper() for col in df.columns]
        logging.info(f"Processing {len(df)} commits from input file.")
        commit_df, failed_urls_df = get_commits(df, BITBUCKET_BASE_URL, use_pygments)
        # Add TOTAL_COMMITS to the input df (grouped by FULL_NAME and month-year of COMMIT_DATE)
        df["TOTAL_COMMITS"] = df.groupby(
            ["FULL_NAME", df["COMMIT_DATE"].dt.strftime("%m-%Y")]
        )["COMMIT_ID"].transform("count")
        if WRITE_SAME_FILE:
            aggregated_df = aggregate_by_commit_id(commit_df)
            if "COMMIT_ID" in df.columns and "COMMIT_ID" in aggregated_df.columns:
                merged_df = pd.merge(df, aggregated_df, on="COMMIT_ID", how="left")
                with pd.ExcelWriter(input_file, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    merged_df.to_excel(writer, sheet_name="Extended Data", index=False)
            else:
                logging.info("COMMIT_ID column missing in either input or aggregated data; skipping merge.")
            if not failed_urls_df.empty:
                with pd.ExcelWriter(input_file, engine="openpyxl", mode="a", if_sheet_exists="overlay") as writer:
                    failed_urls_df.to_excel(writer, sheet_name="Failed URLs", index=False)
        else:
            grouped_commit_df = group_commit_data(commit_df)
            output_file = output_folder / "grouped_commit_df.xlsx"
            with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
                if not grouped_commit_df.empty:
                    grouped_commit_df.to_excel(writer, sheet_name="Grouped Commits", index=False)
                if not failed_urls_df.empty:
                    failed_urls_df.to_excel(writer, sheet_name="Failed URLs", index=False)
                if grouped_commit_df.empty and failed_urls_df.empty:
                    pd.DataFrame({"Message": ["No data available"]}).to_excel(writer, sheet_name="Sheet1", index=False)
        if not failed_urls_df.empty:
            logging.info(f"Failed URLs captured: {len(failed_urls_df)}")
        logging.info("Processing completed")
    except Exception as e:
        logging.error(f"Unexpected error in main: {e}", exc_info=True)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Script terminated due to unexpected error: {e}", exc_info=True)
