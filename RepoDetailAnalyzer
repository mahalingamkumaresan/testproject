#!/usr/bin/env python3
"""
BitbucketRepoAnalyzer – SPK Output Version with 429 Handling

Processes each SPK (project key) from an input Excel, skipping any SPK whose output file already exists.
For each repository under that SPK, it:
  - Uses the top (most recently modified) branch.
  - Retrieves the last commit date on that branch.
  - Fetches all file paths via pagination (limit=1000, at=branch).
  - Optionally uses Pygments to guess file languages for each unique extension; otherwise falls back to extension-based mapping.
  - Applies filename & folder heuristics to detect frameworks/technologies.
All results for an SPK are written to a single Excel file: C:\riskportal\output\{project_key}_repos.xlsx

If any API call returns HTTP 429, the client waits 15 seconds before retrying.
Warnings (e.g. InsecureRequestWarning) are disabled.
"""

import os
import time
import json
import random
import logging
import threading
import requests
import pandas as pd
from datetime import datetime
from openpyxl import load_workbook
from concurrent.futures import ThreadPoolExecutor, as_completed
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound
import urllib3

# Disable insecure request warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ---------------------------------------------------------------------
# Configuration Variables
# ---------------------------------------------------------------------
USE_LEXER_ANALYSIS = True
BITBUCKET_BASE_URL = os.getenv(
    "BITBUCKET_BASE_URL",
    "https://scm.horizon.bankofamerica.com/rest/api/latest"
)
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")
INPUT_EXCEL = r"C:\riskportal\spk.xlsx"
OUTPUT_FOLDER = r"C:\riskportal\output"
TOKEN_BUCKET_CAPACITY = 75
TOKENS_PER_SECOND = 5
MAX_WORKERS = 5

# ---------------------------------------------------------------------
# Logging Configuration
# ---------------------------------------------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        lr = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage(),
        }
        if hasattr(record, "error_category"):
            lr["error_category"] = record.error_category
        if hasattr(record, "severity"):
            lr["severity"] = record.severity
        return json.dumps(lr)

def configure_logging() -> logging.Logger:
    os.makedirs(os.path.join(OUTPUT_FOLDER, "logs"), exist_ok=True)
    log_path = os.path.join(OUTPUT_FOLDER, "logs", "process_log.txt")
    console = logging.StreamHandler()
    console.setFormatter(JsonFormatter())
    fileh = logging.FileHandler(log_path)
    fileh.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console, fileh])
    return logging.getLogger("BitbucketRepoAnalyzer")

logger = configure_logging()

# ---------------------------------------------------------------------
# Token Bucket Rate Limiter
# ---------------------------------------------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float):
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last = time.time()
        self.lock = threading.Lock()

    def consume(self, n: int = 1):
        while True:
            with self.lock:
                now = time.time()
                delta = now - self.last
                refill = delta * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last = now
                if self.tokens >= n:
                    self.tokens -= n
                    return
            time.sleep(0.1)

# ---------------------------------------------------------------------
# Generic API Client with Backoff & 429 Handling
# ---------------------------------------------------------------------
class GenericAPIClient:
    def __init__(self, base_url: str, bucket: TokenBucket, auth: tuple = None):
        self.base_url = base_url.rstrip("/")
        self.bucket = bucket
        self.session = requests.Session()
        if auth:
            self.session.auth = auth
        self.session.verify = False
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.logger = logging.getLogger("GenericAPIClient")

    def make_request(self, endpoint: str, params: dict = None, retries: int = 3, base_delay: float = 1.0):
        url = f"{self.base_url}{endpoint}"
        for attempt in range(retries):
            self.bucket.consume(1)
            try:
                self.logger.debug(f"GET {url} params={params} attempt={attempt+1}")
                r = self.session.get(url, params=params)
                if r.status_code == 429:
                    self.logger.warning(f"429 Too Many Requests for {url}, sleeping 15s")
                    time.sleep(15)
                    continue
                r.raise_for_status()
                return r.json(), None
            except requests.RequestException as e:
                code = r.status_code if 'r' in locals() else 'N/A'
                self.logger.error(
                    f"{url} failed: {e} (status {code})",
                    extra={"error_category": "API Failure", "severity": "error"}
                )
                time.sleep(base_delay * (2**attempt) + random.random())
        return None, "Max retries exceeded"

# ---------------------------------------------------------------------
# Technology Mappings
# ---------------------------------------------------------------------
EXT_LANG_MAP = {
    ".java":"Java", ".kt":"Kotlin", ".sql":"SQL", ".pls":"PL/SQL",
    ".pks":"PL/SQL", ".pkb":"PL/SQL", ".py":"Python", ".js":"JavaScript",
    ".ts":"TypeScript", ".html":"HTML", ".htm":"HTML", ".sh":"Shell Script",
    ".bash":"Shell Script", ".ksh":"Shell Script", ".vbs":"VBScript",
    ".m":"Objective-C", ".swift":"Swift", ".cpp":"C++", ".c":"C",
    ".cs":"C#", ".jsp":"JSP", ".cbl":"COBOL", ".cob":"COBOL"
}
FILE_FRAMEWORK_MAP = {
    "pom.xml":"Maven", "build.gradle":"Gradle", "package.json":"Node.js",
    "requirements.txt":"Python (pip)", "Gemfile":"Ruby on Rails",
    "composer.json":"Composer (PHP)", "Cargo.toml":"Rust (Cargo)",
    "Makefile":"Make", "web.config":".NET Framework", "project.json":".NET Core",
    "struts.xml":"Struts", "applicationContext.xml":"Spring",
    "spring-servlet.xml":"Spring MVC", "hibernate.cfg.xml":"Hibernate",
    "beans.xml":"Java EE (CDI)", "mule-config.xml":"Mule ESB",
    ".twb":"Tableau", ".twbx":"Tableau", "angular.js":"AngularJS",
    "jquery.min.js":"jQuery"
}

# ---------------------------------------------------------------------
# Helpers: Repos, Branch, Commits, Files
# ---------------------------------------------------------------------
def get_repos(client, pk):
    data, err = client.make_request(f"/projects/{pk}/repos", params={"limit":1000})
    if err: return []
    return data.get("values", [])

def get_top_branch(client, pk, slug):
    data, err = client.make_request(f"/projects/{pk}/repos/{slug}/branches",
                                     params={"orderBy":"MODIFICATION","limit":1})
    if err or not data.get("values"):
        logger.warning(f"{pk}/{slug}: falling back to master")
        return "master"
    return data["values"][0]["displayId"]

def get_last_commit_date(client, pk, slug, branch):
    data, err = client.make_request(f"/projects/{pk}/repos/{slug}/commits",
                                     params={"at":branch,"limit":1})
    if err or not data.get("values"):
        return "N/A"
    ts = data["values"][0].get("authorTimestamp",0)
    return datetime.fromtimestamp(ts/1000).strftime("%Y-%m-%d %H:%M:%S") if ts else "N/A"

def get_files_with_pagination(client, pk, slug, branch):
    files=[]; start=0; limit=1000
    while True:
        data, err = client.make_request(
            f"/projects/{pk}/repos/{slug}/files",
            params={"limit":limit,"start":start,"at":branch}
        )
        if err: break
        vals = data.get("values",[])
        files.extend(vals)
        nxt = data.get("nextPageStart")
        if not nxt: break
        start=nxt
    return files

def get_file_content(client, pk, slug, branch, path):
    p = path.lstrip("/")
    client.bucket.consume()
    try:
        r = client.session.get(
            f"{client.base_url}/projects/{pk}/repos/{slug}/raw/{p}",
            params={"at":branch}, verify=False
        )
        if r.status_code == 429:
            time.sleep(15)
            r = client.session.get(f"{client.base_url}/projects/{pk}/repos/{slug}/raw/{p}",
                                   params={"at":branch}, verify=False)
        r.raise_for_status()
        return r.text
    except:
        return ""

# ---------------------------------------------------------------------
# Analyze file list for languages & frameworks
# ---------------------------------------------------------------------
def analyze_file_list(file_list, use_lexer, client, pk, slug, branch):
    uniq = {}
    for fp in file_list:
        ext = os.path.splitext(os.path.basename(fp))[1].lower()
        if ext not in uniq: uniq[ext] = fp

    langs, frs = set(), set()
    if use_lexer:
        for ext, samp in uniq.items():
            c = get_file_content(client, pk, slug, branch, samp)
            if c:
                try:
                    lx = guess_lexer_for_filename("x"+ext, c)
                    langs.add(lx.name)
                except:
                    if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])
            else:
                if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])
    else:
        for ext in uniq:
            if ext in EXT_LANG_MAP: langs.add(EXT_LANG_MAP[ext])

    for fp in file_list:
        bn = os.path.basename(fp)
        if bn in FILE_FRAMEWORK_MAP: frs.add(FILE_FRAMEWORK_MAP[bn])
        if "node_modules" in fp: frs.add("Node.js")
        if "venv" in fp or "env" in fp: frs.add("Python VirtualEnv")
        if "src/main/java" in fp: langs.add("Java")
        if "src/main/resources" in fp: frs.add("Spring MVC")

    exts = [os.path.basename(uniq[e]) if e=="" else e for e in sorted(uniq)]
    return {
        "unique_extensions": exts,
        "languages": sorted(langs),
        "frameworks": sorted(frs),
        "file_count": len(file_list)
    }

# ---------------------------------------------------------------------
# SPK Output Management – One File per SPK
# ---------------------------------------------------------------------
def spk_filepath(pk):
    return os.path.join(OUTPUT_FOLDER, f"{pk}_repos.xlsx")

def process_spk(client, pk):
    out = spk_filepath(pk)
    if os.path.isfile(out):
        logger.info(f"Skipping SPK {pk}: {out} exists")
        return
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    df = pd.DataFrame(columns=[
        "project_key","repo_slug","repo_id","repo_url","repo_name",
        "branch_name","last_commit_date","file_count",
        "unique_extensions","languages","frameworks"
    ])
    for r in get_repos(client, pk):
        slug = r["slug"]
        br = get_top_branch(client, pk, slug)
        lcd= get_last_commit_date(client, pk, slug, br)
        files = get_files_with_pagination(client, pk, slug, br)
        an = analyze_file_list(files, USE_LEXER_ANALYSIS, client, pk, slug, br)
        url=(r.get("links") or {}).get("self",[{}])[0].get("href",
             f"{client.base_url}/projects/{pk}/repos/{slug}")
        rec = {
            "project_key":pk,"repo_slug":slug,"repo_id":r.get("id"),
            "repo_url":url,"repo_name":r.get("name",slug),
            "branch_name":br,"last_commit_date":lcd,
            "file_count":an["file_count"],
            "unique_extensions":", ".join(an["unique_extensions"]),
            "languages":", ".join(an["languages"]),
            "frameworks":", ".join(an["frameworks"])
        }
        df = pd.concat([df, pd.DataFrame([rec])], ignore_index=True)
    df.to_excel(out, index=False)
    logger.info(f"Wrote SPK output: {out}")

def main():
    start = datetime.now()
    logger.info("Starting BitbucketRepoAnalyzer")
    if not os.path.isfile(INPUT_EXCEL):
        logger.error(f"Missing input: {INPUT_EXCEL}")
        return
    wb = load_workbook(INPUT_EXCEL)
    spks = [r[0] for r in wb.active.iter_rows(min_row=2,values_only=True) if r and r[0]]
    bucket = TokenBucket(TOKEN_BUCKET_CAPACITY, TOKENS_PER_SECOND)
    client = GenericAPIClient(
        BITBUCKET_BASE_URL, bucket,
        auth=(BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD)
    )
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        for pk in spks:
            ex.submit(process_spk, client, pk)
    logger.info(f"Completed in {datetime.now()-start}")

if __name__ == "__main__":
    main()
