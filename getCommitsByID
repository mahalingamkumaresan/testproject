#!/usr/bin/env python
import pandas as pd
import requests
import orjson
from openpyxl import load_workbook, Workbook
from datetime import datetime
import logging
import os
import time
import json
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
from logging.handlers import RotatingFileHandler

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

processing_folder = Path("C:/RiskPortal/13MarJavaTrainees")
input_folder = processing_folder / "input"
output_folder = processing_folder / "output"
output_folder.mkdir(parents=True, exist_ok=True)

BITBUCKET_BASE_URL = os.getenv("BITBUCKET_BASE_URL", "https://scm.horizon.bankofamerica.com/rest/api/latest")
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")

WRITE_SAME_FILE = True
use_pygments = True  # Set to True by default

log_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
log_file = output_folder / "BB_data.log"
file_handler = RotatingFileHandler(log_file, maxBytes=5 * 1024 * 1024, backupCount=3)
file_handler.setFormatter(log_formatter)
file_handler.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
console_handler.setLevel(logging.INFO)
logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])

FILE_TYPE_CACHE: Dict[str, str] = {}

session = requests.Session()
session.auth = (BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD)  # Basic Authentication
session.verify = False

def identify_file_type(file_name: str, use_pygments: bool = False) -> str:
    try:
        if use_pygments:
            if file_name in FILE_TYPE_CACHE:
                return FILE_TYPE_CACHE[file_name]
            try:
                lexer = guess_lexer_for_filename(file_name, "")
                FILE_TYPE_CACHE[file_name] = lexer.name
                return lexer.name
            except ClassNotFound:
                return "UNKNOWN"
        return "NOT USED"
    except Exception as e:
        logging.error(f"Error in identify_file_type for {file_name}: {e}")
        return "UNKNOWN"

def make_request_with_retry(url: str, retries: int = 3, delay: int = 2) -> Tuple[Optional[requests.Response], Optional[str]]:
    """
    Calls session.get(url) with exponential backoff on rate-limit errors (429).
    Other errors are logged and returned immediately.
    """
    for attempt in range(retries):
        try:
            logging.debug(f"Requesting: {url} (Attempt {attempt+1}/{retries})")
            response = session.get(url)
            response.raise_for_status()
            return response, None
        except requests.exceptions.RequestException as e:
            error_message = str(e)
            if hasattr(e, "response") and e.response is not None:
                status = e.response.status_code
                if status == 401:
                    logging.error(f"Received 401 Unauthorized for URL {url}")
                if status == 429:
                    logging.error(f"Rate limit error on {url}: {error_message} (Attempt {attempt+1}/{retries})")
                    if attempt < retries - 1:
                        time.sleep(delay * (2 ** attempt))
                    else:
                        return None, error_message
                else:
                    logging.error(f"Error on {url}: {error_message} (Status code: {status}, not retrying)")
                    return None, error_message
            else:
                logging.error(f"Error on {url}: {error_message} (No response object)")
                return None, error_message
    return None, "Unknown error"

def get_committer_info(project_key: str, repo_slug: str, commit_id: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """
    Calls /commits/{commit_id} to retrieve committer details (name and email).
    Returns (committer_name, committer_email, error_reason).
    """
    url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}"
    resp, error_reason = make_request_with_retry(url)
    if not resp:
        return None, None, error_reason
    try:
        data = orjson.loads(resp.text)
        name = data.get("committer", {}).get("name")
        email = data.get("committer", {}).get("emailAddress")
        return name, email, None
    except Exception as e:
        logging.error(f"Error decoding JSON for commit {commit_id} at URL {url}: {e}")
        return None, None, str(e)

def process_diff(diff: Dict[str, Any], file_name: str) -> Tuple[int, int, int]:
    """
    Parses a single file diff to count lines added, removed, and modified.
    """
    try:
        if diff.get("binary", False) or diff.get("source", {}).get("truncated", False):
            return 0, 0, 0
        lines_added = 0
        lines_removed = 0
        lines_modified = 0
        removed_lines: Dict[Any, str] = {}
        added_lines: Dict[Any, str] = {}
        dest_name = diff.get("destination", {}).get("name")
        source_name = diff.get("source", {}).get("name") if diff.get("source") else None
        if file_name not in {dest_name, source_name}:
            return 0, 0, 0
        for hunk in diff.get("hunks", []):
            for segment in hunk.get("segments", []):
                seg_type = segment.get("type")
                if seg_type == "REMOVED":
                    for line in segment.get("lines", []):
                        line_number = line.get("source")
                        removed_lines[line_number] = line.get("line", "").strip()
                        lines_removed += 1
                elif seg_type == "ADDED":
                    for line in segment.get("lines", []):
                        line_number = line.get("destination")
                        added_lines[line_number] = line.get("line", "").strip()
                        lines_added += 1
        # Detect modifications
        for key, removed_line in removed_lines.items():
            if key in added_lines and added_lines[key] != removed_line:
                lines_modified += 1
                lines_added -= 1
                lines_removed -= 1
        return lines_added, lines_removed, lines_modified
    except Exception as e:
        logging.error(f"Error in process_diff for file {file_name}. Error: {e}")
        return 0, 0, 0

def get_commits(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Processes each commit row. 
    - For merge commits: calls /commits/{commit_id}, extracts committer info, no diff parsing.
    - For non-merge commits: 
        1) calls /commits/{commit_id} for committer info
        2) calls /commits/{commit_id}/diff?ignore_whitespace=true for diffs
        3) if diffs are empty => fail
    Returns (commit_df, failed_df).
    """
    if df.empty:
        logging.warning("Input file is empty! Exiting.")
        return pd.DataFrame(), pd.DataFrame()

    failed_urls = []
    commit_details = []

    def process_commit(index: int, row: pd.Series) -> None:
        commit_id = row["COMMIT_ID"]
        project_key = row["SPK"]
        repo_slug = row["REPOSITORY_NAME"]
        commit_type = str(row.get("COMMIT_TYPE", "")).strip().lower()
        full_name = row["FULL_NAME"]
        email_id = row["EMAIL_ID"]
        branch_name = row["BRANCH_NAME"]
        commit_date = pd.to_datetime(row["COMMIT_DATE"])
        commit_month_year = commit_date.strftime("%m-%Y")
        is_merge_commit = (commit_type == "merge")

        # 1) Retrieve committer info from /commits/{commit_id}
        committer_name, committer_email, committer_error = get_committer_info(project_key, repo_slug, commit_id)
        if committer_error:
            failed_urls.append((
                f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}",
                commit_id,
                f"Could not retrieve committer info: {committer_error}"
            ))
            return

        if is_merge_commit:
            # Merge => just store data from /commits/{commit_id}, no diff.
            effective_author_name = committer_name if committer_name else full_name
            effective_author_email = committer_email if committer_email else email_id
            commit_details.append({
                "COMMIT_ID": commit_id,
                "SPK": project_key,
                "REPOSITORY_NAME": repo_slug,
                "AUTHOR_NAME": effective_author_name,
                "AUTHOR_EMAIL": effective_author_email,
                "COMMITTER_NAME": committer_name or "",
                "COMMITTER_EMAIL": committer_email or "",
                "COMMIT_MONTH_YEAR": commit_month_year,
                "BRANCH_NAME": branch_name,
                "IS_MERGE_COMMIT": True,
                "COMMIT_TYPE": commit_type,
                "FILE_NAME": "Merge Commit",
                "FILE_TYPE": "N/A",
                "LINES_ADDED": 0,
                "LINES_REMOVED": 0,
                "LINES_MODIFIED": 0,
                "DIFF_URL": f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}"
            })
        else:
            # Non-merge => also call /commits/{commit_id}/diff?ignore_whitespace=true
            effective_author_name = committer_name if committer_name else full_name
            effective_author_email = committer_email if committer_email else email_id
            diff_url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
            diff_response, diff_err = make_request_with_retry(diff_url)
            if not diff_response:
                # If we cannot retrieve diff, mark as failed
                failed_urls.append((diff_url, commit_id, f"Could not retrieve diff: {diff_err}"))
                return
            try:
                diff_data = orjson.loads(diff_response.text)
            except Exception as e:
                logging.error(f"Error decoding JSON for commit {commit_id} at URL {diff_url}: {e}")
                failed_urls.append((diff_url, commit_id, f"Error decoding diff JSON: {str(e)}"))
                return

            diffs = diff_data.get("diffs")
            if not diffs:
                # If diffs is empty or None => fail
                reason = "No difference found or inaccessible commit"
                failed_urls.append((diff_url, commit_id, reason))
                return

            # Parse each diff
            for diff in diffs:
                file_name = diff.get("source", {}).get("name", "UNKNOWN")
                file_type = identify_file_type(file_name, use_pygments)
                la, lr, lm = process_diff(diff, file_name)
                commit_details.append({
                    "COMMIT_ID": commit_id,
                    "SPK": project_key,
                    "REPOSITORY_NAME": repo_slug,
                    "AUTHOR_NAME": effective_author_name,
                    "AUTHOR_EMAIL": effective_author_email,
                    "COMMITTER_NAME": committer_name or "",
                    "COMMITTER_EMAIL": committer_email or "",
                    "COMMIT_MONTH_YEAR": commit_month_year,
                    "BRANCH_NAME": branch_name,
                    "IS_MERGE_COMMIT": False,
                    "COMMIT_TYPE": commit_type,
                    "FILE_NAME": file_name,
                    "FILE_TYPE": file_type,
                    "LINES_ADDED": la,
                    "LINES_REMOVED": lr,
                    "LINES_MODIFIED": lm,
                    "DIFF_URL": diff_url
                })

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_commit, idx, row) for idx, row in df.iterrows()]
        for i, future in enumerate(as_completed(futures), start=1):
            logging.info(f"Processed commit {i} of {len(df)}")

    commit_df = pd.DataFrame(commit_details)
    # Build a failed_df with 3 columns: "Failed URL", "CommitID", "Failure Reason"
    failed_df = pd.DataFrame(failed_urls, columns=["Failed URL", "CommitID", "Failure Reason"]) if failed_urls else pd.DataFrame()
    return commit_df, failed_df

def group_commit_data(commit_df: pd.DataFrame) -> pd.DataFrame:
    try:
        if commit_df.empty:
            return pd.DataFrame()
        grouped_df = commit_df.groupby([
            "SPK", "REPOSITORY_NAME", "AUTHOR_NAME", "AUTHOR_EMAIL", "COMMITTER_NAME",
            "COMMITTER_EMAIL", "COMMIT_MONTH_YEAR", "BRANCH_NAME", "IS_MERGE_COMMIT",
            "FILE_NAME", "FILE_TYPE"
        ]).agg({
            "LINES_ADDED": "sum",
            "LINES_REMOVED": "sum",
            "LINES_MODIFIED": "sum",
            "COMMIT_ID": "count",
            "DIFF_URL": lambda x: "; ".join(x.astype(str).unique())
        }).reset_index()
        grouped_df.rename(columns={"COMMIT_ID": "GROUP_COMMITS"}, inplace=True)
        total_df = commit_df.groupby(["AUTHOR_NAME", "COMMIT_MONTH_YEAR", "FILE_NAME"])["COMMIT_ID"]\
                            .count().reset_index(name="TOTAL_COMMITS")
        merged_df = pd.merge(grouped_df, total_df, on=["AUTHOR_NAME", "COMMIT_MONTH_YEAR", "FILE_NAME"], how="left")
        return merged_df
    except Exception as e:
        logging.error(f"Error in group_commit_data: {e}")
        return pd.DataFrame()

def aggregate_by_commit_id(commit_df: pd.DataFrame) -> pd.DataFrame:
    if commit_df.empty:
        return commit_df
    agg_funcs = {
        "SPK": "first",
        "REPOSITORY_NAME": "first",
        "AUTHOR_NAME": "first",
        "AUTHOR_EMAIL": "first",
        "COMMITTER_NAME": "first",
        "COMMITTER_EMAIL": "first",
        "COMMIT_MONTH_YEAR": "first",
        "BRANCH_NAME": "first",
        "IS_MERGE_COMMIT": "first",
        "COMMIT_TYPE": "first",
        "FILE_NAME": lambda x: "; ".join(x.astype(str).unique()),
        "FILE_TYPE": lambda x: "; ".join(x.astype(str).unique()),
        "LINES_ADDED": "sum",
        "LINES_REMOVED": "sum",
        "LINES_MODIFIED": "sum",
        "DIFF_URL": lambda x: "; ".join(x.astype(str).unique())
    }
    aggregated = commit_df.groupby("COMMIT_ID").agg(agg_funcs).reset_index()
    return aggregated

def main() -> None:
    try:
        input_file = input_folder / "CommitInputs.xlsx"
        df = pd.read_excel(input_file)
        df.columns = [col.strip().upper() for col in df.columns]
        logging.info(f"Processing {len(df)} commits from input file.")

        # Call get_commits, which excludes failed commits from commit_df
        commit_df, failed_urls_df = get_commits(df)

        # Before we do extended data, let's add "TOTAL_COMMITS" to the input
        if "FULL_NAME" in df.columns and "COMMIT_DATE" in df.columns:
            df["TOTAL_COMMITS"] = df.groupby(
                ["FULL_NAME", df["COMMIT_DATE"].dt.strftime("%m-%Y")]
            )["COMMIT_ID"].transform("count")

        if WRITE_SAME_FILE:
            aggregated_df = aggregate_by_commit_id(commit_df)
            if "COMMIT_ID" in df.columns and "COMMIT_ID" in aggregated_df.columns:
                merged_df = pd.merge(df, aggregated_df, on="COMMIT_ID", how="left")
                with pd.ExcelWriter(input_file, engine="openpyxl", mode="a", if_sheet_exists="replace") as writer:
                    merged_df.to_excel(writer, sheet_name="Extended Data", index=False)
            else:
                logging.info("COMMIT_ID column missing in either input or aggregated data; skipping merge.")
            if not failed_urls_df.empty:
                with pd.ExcelWriter(input_file, engine="openpyxl", mode="a", if_sheet_exists="overlay") as writer:
                    failed_urls_df.to_excel(writer, sheet_name="Failed URLs", index=False)
        else:
            grouped_commit_df = group_commit_data(commit_df)
            output_file = output_folder / "grouped_commit_df.xlsx"
            with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
                if not grouped_commit_df.empty:
                    grouped_commit_df.to_excel(writer, sheet_name="Grouped Commits", index=False)
                if not failed_urls_df.empty:
                    failed_urls_df.to_excel(writer, sheet_name="Failed URLs", index=False)
                if grouped_commit_df.empty and failed_urls_df.empty:
                    pd.DataFrame({"Message": ["No data available"]}).to_excel(writer, sheet_name="Sheet1", index=False)

        if not failed_urls_df.empty:
            logging.info(f"Failed URLs captured: {len(failed_urls_df)}")

        logging.info("Processing completed")
    except Exception as e:
        logging.error(f"Unexpected error in main: {e}", exc_info=True)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"Script terminated due to unexpected error: {e}", exc_info=True)
###############################
