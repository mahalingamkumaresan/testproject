import re
import threading
import logging
import json
import time
import os
import glob
import random
import pandas as pd
import requests
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from openpyxl import load_workbook
from requests.adapters import HTTPAdapter
from typing import Tuple, Optional, List, Dict, Set

# Suppress InsecureRequestWarning.
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)
# Suppress logs from urllib3.
logging.getLogger("urllib3").setLevel(logging.CRITICAL)
# Set CommitProcessor logger to WARNING.
logging.getLogger("CommitProcessor").setLevel(logging.WARNING)

# ------------------------------
# Custom JSON Logging Formatter
# ------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

def configure_logging(logs_folder: str) -> logging.Logger:
    os.makedirs(logs_folder, exist_ok=True)
    output_log_path = os.path.join(logs_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(output_log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    return logging.getLogger("Main")

main_logger: Optional[logging.Logger] = None  # Will be set in main().

# ------------------------------
# Utility: Load processed commit IDs from existing Parquet files.
# ------------------------------
def load_processed_commit_ids(output_folder: str) -> Set[str]:
    processed: Set[str] = set()
    for file in glob.glob(os.path.join(output_folder, "commit_output_part_*.parquet")):
        try:
            df_existing = pd.read_parquet(file, engine="pyarrow")
            if "COMMIT_ID" in df_existing.columns:
                processed.update(df_existing["COMMIT_ID"].unique())
            elif "Commit Id" in df_existing.columns:
                processed.update(df_existing["Commit Id"].unique())
        except Exception as e:
            main_logger.error(f"Error reading {file}: {e}")
    return processed

# ------------------------------
# Helper: Sanitize map objects before writing to Parquet.
# ------------------------------
def sanitize_map_objects(value):
    if isinstance(value, map):
        return list(value)
    return value

def sanitize_rows(rows: List[Dict]) -> List[Dict]:
    return [{k: sanitize_map_objects(v) for k, v in row.items()} for row in rows]

# ------------------------------
# Token Bucket for Rate Limiting with Exponential Backoff
# ------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float) -> None:
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1) -> None:
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ------------------------------
# Bitbucket API Wrapper with Increased Connection Pool and Exponential Backoff
# ------------------------------
class BitbucketAPI:
    def __init__(self, base_url: str, username: str, app_password: str, token_bucket: TokenBucket) -> None:
        self.base_url = base_url
        self.username = username
        self.app_password = app_password
        self.token_bucket = token_bucket
        self.session = requests.Session()
        self.session.auth = (self.username, self.app_password)
        self.session.verify = False
        adapter = HTTPAdapter(pool_connections=200, pool_maxsize=200)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        self.logger = logging.getLogger("BitbucketAPI")

    def make_request(self, url: str, retries: int = 2, base_delay: int = 2) -> Tuple[Optional[requests.Response], Optional[str]]:
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {url} (Attempt {attempt+1}/{retries})")
                response = self.session.get(url)
                response.raise_for_status()
                return response, None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                status_code = response.status_code if response is not None else "N/A"
                if response is not None and response.status_code == 429:
                    self.logger.critical(
                        f"429 Too Many Requests encountered for {url}!",
                        extra={"error_category": "Rate Limit", "severity": "critical"}
                    )
                extra = {"error_category": "API Failure", "severity": "error"}
                self.logger.error(f"Request failed for {url}: {error_message} (Status: {status_code})", extra=extra)
                if response is not None and response.status_code in [404, 401]:
                    try:
                        error_message = response.json().get('errors', [{}])[0].get('message', error_message)
                        return None, error_message
                    except Exception:
                        pass
                sleep_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                time.sleep(sleep_time)
                if response is not None and response.status_code == 429:
                    time.sleep(base_delay * 10)
                else:
                    continue
        return None, "Max retries exceeded"

# ------------------------------
# Helper: Diff Parsing from API Response.
# ------------------------------
def parse_diff_change(change: dict) -> Tuple[int, int, int]:
    lines_added = 0
    lines_removed = 0
    lines_modified = 0
    try:
        for hunk in change.get('hunks', []):
            for segment in hunk.get('segments', []):
                if segment.get('type') == 'REMOVED':
                    for _ in segment.get('lines', []):
                        lines_removed += 1
                elif segment.get('type') == 'ADDED':
                    for _ in segment.get('lines', []):
                        lines_added += 1
    except Exception as e:
        logging.getLogger("DiffParser").error(f"Error parsing diff change: {e}")
        raise e
    return lines_added, lines_removed, lines_modified

# ------------------------------
# SPK Mode: Commit Processor (Commit Collector)
# ------------------------------
class CommitProcessor:
    def __init__(self, api: BitbucketAPI, output_folder: str, author_email_filter: Optional[Set[str]] = None, recent_branches: int = 1) -> None:
        # In SPK mode, we process only the top branch.
        self.api = api
        self.output_folder = output_folder
        self.author_email_filter = author_email_filter
        self.recent_branches = 1  # Force one branch.
        self.logger = logging.getLogger("CommitProcessor")
        self.failed_urls: List[Dict] = []
        self.diff_failures: List[Dict] = []

    def get_top_branch(self, project_key: str, repo_slug: str) -> Optional[str]:
        url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/branches?orderBy=MODIFICATION&limit=1"
        response, err = self.api.make_request(url)
        if not response:
            self.failed_urls.append({
                "url": url,
                "reason": err,
                "error_category": "API Failure",
                "severity": "error"
            })
            return None
        branches = response.json().get("values", [])
        if not branches:
            return None
        return branches[0].get("displayId")

    def process_commits_spk(self, project_key: str, repo_slug: str) -> List[Dict]:
        top_branch = self.get_top_branch(project_key, repo_slug)
        if not top_branch:
            return []
        output_rows: List[Dict] = []
        page = 0
        next_page: Optional[int] = None
        while page < 2:
            if next_page is not None:
                url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits?limit=1000&start={next_page}&until={top_branch}"
            else:
                url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits?limit=1000&until={top_branch}"
            response, error_reason = self.api.make_request(url)
            if not response:
                self.failed_urls.append({
                    "url": url,
                    "reason": error_reason,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                break
            data = response.json()
            for commit in data.get("values", []):
                # For merge commits, check if more than one parent.
                if len(commit.get("parents", [])) > 1:
                    author = commit.get("author", {})
                    author_name = author.get("displayName") or author.get("name") or "Unknown Author"
                    author_email = (author.get("emailAddress") or "").lower()
                    row = {
                        "Project Key": project_key,
                        "Repo Slug": repo_slug,
                        "Author Name": author_name,
                        "Author Email": author_email,
                        "Commit Month": datetime.fromtimestamp(commit["authorTimestamp"]/1000).strftime("%Y-%m"),
                        "Commit Id": commit.get("id", ""),
                        "File Name": "merge_commit",
                        "Month Lines Added": 0,
                        "Month Lines Removed": 0,
                        "Month Lines Modified": 0,
                        "Total Commits": 1,
                        "File Status": "merge"
                    }
                    output_rows.append(row)
                else:
                    commit_id = commit.get("id", "")
                    author = commit.get("author", {})
                    author_name = author.get("displayName") or author.get("name") or "Unknown Author"
                    author_email = (author.get("emailAddress") or "").lower()
                    commit_month = datetime.fromtimestamp(commit["authorTimestamp"]/1000).strftime("%Y-%m")
                    diff_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
                    file_response, diff_error = self.api.make_request(diff_url)
                    if not file_response:
                        self.failed_urls.append({
                            "url": diff_url,
                            "reason": diff_error,
                            "error_category": "API Failure",
                            "severity": "error"
                        })
                        continue
                    try:
                        diff_data = file_response.json()
                    except Exception as e:
                        self.diff_failures.append({
                            "url": diff_url,
                            "commit_id": commit_id,
                            "reason": str(e),
                            "error_category": "Diff Parsing Failure",
                            "severity": "error"
                        })
                        continue
                    first_row = True
                    for change in diff_data.get("diffs", []):
                        source_info = change.get("source", {})
                        dest_info = change.get("destination", {})
                        file_name = (dest_info.get("name") if dest_info and dest_info.get("name")
                                     else source_info.get("name", "Unknown"))
                        file_status = "modified"
                        if not source_info:
                            file_status = "new"
                        elif not dest_info:
                            file_status = "deleted"
                        try:
                            added, removed, modified = parse_diff_change(change)
                        except Exception as e:
                            self.diff_failures.append({
                                "url": diff_url,
                                "commit_id": commit_id,
                                "reason": f"Exception during diff parsing: {e}",
                                "error_category": "Diff Parsing Failure",
                                "severity": "error"
                            })
                            continue
                        row = {
                            "Project Key": project_key,
                            "Repo Slug": repo_slug,
                            "Author Name": author_name,
                            "Author Email": author_email,
                            "Commit Month": commit_month,
                            "Commit Id": commit_id,
                            "File Name": file_name,
                            "Month Lines Added": added,
                            "Month Lines Removed": removed,
                            "Month Lines Modified": modified,
                            "Total Commits": 1 if first_row else 0,
                            "File Status": file_status
                        }
                        output_rows.append(row)
                        first_row = False
            next_page = data.get("nextPageStart")
            if not next_page:
                break
            page += 1
        return output_rows

    def write_chunk(self, rows: List[Dict], chunk_index: int, filename_prefix: Optional[str] = None) -> None:
        rows = sanitize_rows(rows)
        if rows:
            df = pd.DataFrame(rows)
            if filename_prefix:
                output_path = os.path.join(self.output_folder, f"{filename_prefix}.parquet")
            else:
                output_path = os.path.join(self.output_folder, f"commit_output_part_{chunk_index}.parquet")
            if os.path.exists(output_path):
                base, ext = os.path.splitext(output_path)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = f"{base}_{timestamp}{ext}"
            df.to_parquet(output_path, engine="pyarrow", index=False)
            self.logger.info(f"Output written with {len(rows)} rows to {output_path}")
        else:
            self.logger.info(f"No rows to write for chunk {chunk_index}.")

    def write_failure_files(self) -> None:
        logs_folder = os.path.join(os.path.dirname(self.output_folder), "logs")
        os.makedirs(logs_folder, exist_ok=True)
        if self.failed_urls:
            df_failed = pd.DataFrame(self.failed_urls)
            failed_path = os.path.join(logs_folder, "failed_urls.parquet")
            df_failed.to_parquet(failed_path, engine="pyarrow", index=False)
            self.logger.info(f"Failed URLs written to {failed_path}")
        if self.diff_failures:
            df_diff_fail = pd.DataFrame(self.diff_failures)
            diff_fail_path = os.path.join(logs_folder, "diff_failures.parquet")
            df_diff_fail.to_parquet(diff_fail_path, engine="pyarrow", index=False)
            self.logger.info(f"Diff failures written to {diff_fail_path}")

# ------------------------------
# SPK Mode Main: Process each SPK and its repositories.
# ------------------------------
def process_spk_mode(input_folder: str, output_folder: str, start_date: str, end_date: str) -> None:
    project_keys_file = os.path.join(input_folder, 'WTMSPK.xlsx')
    wb = load_workbook(project_keys_file)
    sheet = wb.active
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    total_spks = len(spk_rows)
    spk_progress = 0

    filter_df = pd.read_excel(os.path.join(input_folder, "author_filter.xlsx"))
    author_email_filter = set(filter_df['Author Email'].str.lower())

    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    token_bucket = TokenBucket(capacity=75, tokens_per_second=5)
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    # For SPK mode, process only one top branch.
    processor = CommitProcessor(api, output_folder, author_email_filter, recent_branches=1)

    futures: Dict[str, any] = {}
    with ThreadPoolExecutor(max_workers=5) as executor:
        for row in spk_rows:
            project_key = row[0]
            if not project_key:
                continue
            # Process each project individually (repo-level check will be done later).
            futures[project_key] = executor.submit(process_project_spk, project_key, processor, start_date, end_date)
        for future in as_completed(futures):
            spk_progress += 1
            proj_key, repo_outputs = future.result()
            main_logger.info(f"Finished processing project {proj_key} ({spk_progress}/{total_spks})")
    processor.write_failure_files()
    main_logger.info(f"Total tokens consumed: {token_bucket.calls_made}")
    main_logger.info(f"Remaining token capacity: {token_bucket.tokens}")

def process_project_spk(project_key: str, processor: CommitProcessor, start_date: str, end_date: str) -> Tuple[str, List[Tuple[str, List[Dict]]]]:
    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    api = processor.api
    repo_outputs: List[Tuple[str, List[Dict]]] = []
    repos_url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos?limit=1000"
    response, error_reason = api.make_request(repos_url)
    if not response:
        processor.failed_urls.append({
            "url": repos_url,
            "reason": error_reason,
            "error_category": "API Failure",
            "severity": "error"
        })
        return (project_key, repo_outputs)
    repos_data = response.json()
    for repo in repos_data.get('values', []):
        repo_slug = repo.get('slug')
        if not repo_slug:
            continue
        # Check if output file for this repo already exists.
        output_file = os.path.join(processor.output_folder, f"output_{project_key}_{repo_slug}.parquet")
        if os.path.exists(output_file):
            main_logger.info(f"Output for project {project_key} repo {repo_slug} exists; skipping.")
            continue
        rows = processor.process_commits_spk(project_key, repo_slug)
        repo_outputs.append((repo_slug, rows))
    return (project_key, repo_outputs)

# ------------------------------
# Commit Mode Main: Process commit IDs in chunks.
# ------------------------------
def process_commit_mode(input_folder: str, output_folder: str, chunk_size: int = 1000) -> None:
    commit_id_file = os.path.join(input_folder, 'commit_ids.xlsx')
    df = pd.read_excel(commit_id_file)
    processed_commits = load_processed_commit_ids(output_folder)
    df = df[~df["COMMIT_ID"].isin(processed_commits)]
    total_commits = len(df)
    main_logger.info(f"Total commit IDs to process: {total_commits}")
    
    token_bucket = TokenBucket(capacity=75, tokens_per_second=5)
    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    processor = CommitProcessor(api, output_folder)
    
    processed_commit_count = 0
    count_lock = threading.Lock()
    
    def process_chunk(chunk_df: pd.DataFrame, chunk_index: int) -> None:
        nonlocal processed_commit_count
        rows = process_commit_chunk(processor, chunk_df)
        filename = f"commit_output_part_{chunk_index}"
        processor.write_chunk(rows, chunk_index, filename_prefix=filename)
        with count_lock:
            processed_commit_count += len(chunk_df)
            current = processed_commit_count
        main_logger.info(f"Processed {current} out of {total_commits} commit IDs (Chunk {chunk_index}: Output file: {filename}.parquet)")
        main_logger.info(f"Currently, {threading.active_count()} threads are active.")
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for i in range(0, total_commits, chunk_size):
            chunk_df = df.iloc[i:i+chunk_size]
            futures.append(executor.submit(process_chunk, chunk_df, i // chunk_size + 1))
        for future in as_completed(futures):
            future.result()
    
    processor.write_failure_files()
    main_logger.info(f"Total tokens consumed: {token_bucket.calls_made}")
    main_logger.info(f"Remaining token capacity: {token_bucket.tokens}")

def process_commit_chunk(processor: CommitProcessor, chunk_df: pd.DataFrame) -> List[Dict]:
    output_rows: List[Dict] = []
    for index, row in chunk_df.iterrows():
        commit_type = row.get("COMMIT_TYPE", "").lower()
        commit_id = row["COMMIT_ID"]
        project_key = row["SPK"]
        repo_slug = row["REPOSITORY_NAME"]
        if "AUTHOR_TIMESTAMP" in row and pd.notna(row["AUTHOR_TIMESTAMP"]):
            try:
                commit_date = datetime.fromtimestamp(float(row["AUTHOR_TIMESTAMP"]) / 1000)
                commit_month = commit_date.strftime("%Y-%m")
            except Exception:
                commit_month = "Unknown"
        else:
            commit_month = "Unknown"
        if commit_type == "merge":
            author_name = row.get("FULL_NAME", "Unknown")
            author_email = row.get("EMAIL_ID", "Unknown").lower()
            out_row = {
                "Project Key": project_key,
                "Repo Slug": repo_slug,
                "Author Name": author_name,
                "Author Email": author_email,
                "Commit Month": commit_month,
                "Commit Id": commit_id,
                "File Name": "merge_commit",
                "Month Lines Added": 0,
                "Month Lines Removed": 0,
                "Month Lines Modified": 0,
                "Total Commits": 1,
                "File Status": "merge"
            }
            output_rows.append(out_row)
            continue

        processor.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug} (API mode)")
        diff_url = f"{processor.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
        file_response, diff_error = processor.api.make_request(diff_url)
        if not file_response:
            processor.failed_urls.append({
                "url": diff_url,
                "reason": diff_error,
                "error_category": "API Failure",
                "severity": "error"
            })
            # If diff call fails, write row with 0 changes.
            out_row = {
                "Project Key": project_key,
                "Repo Slug": repo_slug,
                "Author Name": row.get("FULL_NAME", "Unknown"),
                "Author Email": row.get("EMAIL_ID", "Unknown").lower(),
                "Commit Month": commit_month,
                "Commit Id": commit_id,
                "File Name": "",
                "Month Lines Added": 0,
                "Month Lines Removed": 0,
                "Month Lines Modified": 0,
                "Total Commits": 1,
                "File Status": "no_diff"
            }
            output_rows.append(out_row)
            continue
        try:
            diff_data = file_response.json()
        except Exception as e:
            processor.diff_failures.append({
                "url": diff_url,
                "commit_id": commit_id,
                "reason": str(e),
                "error_category": "Diff Parsing Failure",
                "severity": "error"
            })
            continue
        first_row = True
        if 'diffs' not in diff_data or not diff_data['diffs']:
            out_row = {
                "Project Key": project_key,
                "Repo Slug": repo_slug,
                "Author Name": row.get("FULL_NAME", "Unknown"),
                "Author Email": row.get("EMAIL_ID", "Unknown").lower(),
                "Commit Month": commit_month,
                "Commit Id": commit_id,
                "File Name": "",
                "Month Lines Added": 0,
                "Month Lines Removed": 0,
                "Month Lines Modified": 0,
                "Total Commits": 1,
                "File Status": "no_diff"
            }
            output_rows.append(out_row)
            continue
        for change in diff_data.get("diffs", []):
            source_info = change.get('source', {})
            dest_info = change.get('destination', {})
            file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                         else source_info.get('name', 'Unknown'))
            file_status = "modified"
            if not source_info:
                file_status = "new"
            elif not dest_info:
                file_status = "deleted"
            try:
                added, removed, modified = parse_diff_change(change)
                if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                    processor.diff_failures.append({
                        "url": diff_url,
                        "commit_id": commit_id,
                        "reason": "No changes found in diff",
                        "error_category": "Diff Parsing Failure",
                        "severity": "warning"
                    })
            except Exception as e:
                processor.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": f"Exception during diff parsing: {e}",
                    "error_category": "Diff Parsing Failure",
                    "severity": "error"
                })
                continue
            out_row = {
                "Project Key": project_key,
                "Repo Slug": repo_slug,
                "Author Name": row.get("FULL_NAME", "Unknown"),
                "Author Email": row.get("EMAIL_ID", "Unknown").lower(),
                "Commit Month": commit_month,
                "File Name": file_name,
                "Month Lines Added": added,
                "Month Lines Removed": removed,
                "Month Lines Modified": modified,
                "Total Commits": 1 if first_row else 0,
                "Commit Id": commit_id,
                "File Status": file_status
            }
            output_rows.append(out_row)
            first_row = False
    return output_rows

# ------------------------------
# Main Entry Point for SPK and Commit Modes.
# ------------------------------
def main() -> None:
    parser = argparse.ArgumentParser(description="Process Bitbucket commits (API-only).")
    parser.add_argument('--mode', choices=['spk', 'commit'], default='commit',
                        help="Input mode: 'spk' for SPK list mode, 'commit' for commit id mode. (Default: commit)")
    parser.add_argument('--start_date', default='2024-01-01',
                        help="Start date (YYYY-MM-DD) for commit filtering (used in SPK mode).")
    parser.add_argument('--end_date', default='2025-03-31',
                        help="End date (YYYY-MM-DD) for commit filtering (used in SPK mode).")
    parser.add_argument('--recent_branches', type=int, default=1,
                        help="(SPK mode) Number of top branches to process (default: 1).")
    parser.add_argument('--order', choices=['top-down', 'bottom-up'], default='top-down',
                        help="Processing order for SPKs (top-down or bottom-up).")
    parser.add_argument('--commit_chunk_size', type=int, default=1000,
                        help="Chunk size for processing commit mode (default: 1000).")
    args = parser.parse_args()

    common_folder = "C:/Riskportal/3Mar2025_JavaTraines"  # Adjust as needed.
    input_folder = os.path.join(common_folder, "input")
    output_folder = os.path.join(common_folder, "output")
    logs_folder = os.path.join(common_folder, "logs")
    os.makedirs(logs_folder, exist_ok=True)
    
    # Configure logging to write logs in the logs folder.
    for handler in logging.getLogger().handlers[:]:
        logging.getLogger().removeHandler(handler)
    output_log_path = os.path.join(logs_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(output_log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    global main_logger
    main_logger = logging.getLogger("Main")

    if args.mode == 'spk':
        main_logger.info("Running in SPK mode (processing top branch, two pages max).")
        process_spk_mode(input_folder, output_folder, args.start_date, args.end_date)
    elif args.mode == 'commit':
        main_logger.info("Running in Commit Id mode.")
        process_commit_mode(input_folder, output_folder, args.commit_chunk_size)

if __name__ == "__main__":
    main()
# # #
