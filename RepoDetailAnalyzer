import os
import time
import json
import random
import logging
import threading
import requests
import pandas as pd
from datetime import datetime
from openpyxl import load_workbook
from concurrent.futures import ThreadPoolExecutor, as_completed
from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound

# ---------------------------------------------------------------------
# Configuration Variables
# ---------------------------------------------------------------------
USE_LEXER_ANALYSIS = True   # Set to True to use real lexer analysis.
BITBUCKET_BASE_URL = os.getenv("BITBUCKET_BASE_URL", "https://scm.example.com/rest/api/1.0")
BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")
INPUT_EXCEL = r"C:\riskportal\WTMSPK.xlsx"
OUTPUT_FOLDER = r"C:\riskportal"  # All outputs will be written here.
TOKEN_BUCKET_CAPACITY = 75
TOKENS_PER_SECOND = 5
MAX_WORKERS = 5
MAX_REPOS_PER_SPK = 10  # Set to None to process all repos per SPK.

# ---------------------------------------------------------------------
# Logging Configuration
# ---------------------------------------------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

def configure_logging() -> logging.Logger:
    logs_folder = os.path.join(OUTPUT_FOLDER, "logs")
    os.makedirs(logs_folder, exist_ok=True)
    log_path = os.path.join(logs_folder, "process_log.txt")

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())

    file_handler = logging.FileHandler(log_path)
    file_handler.setFormatter(JsonFormatter())

    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    return logging.getLogger("BitbucketRepoAnalyzer")

logger = configure_logging()

# ---------------------------------------------------------------------
# Rate Limiting with Token Bucket
# ---------------------------------------------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float) -> None:
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1) -> None:
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ---------------------------------------------------------------------
# Generic API Client with Exponential Backoff
# ---------------------------------------------------------------------
class GenericAPIClient:
    def __init__(self, base_url: str, token_bucket: TokenBucket, auth: tuple = None):
        self.base_url = base_url.rstrip("/")
        self.token_bucket = token_bucket
        self.auth = auth
        self.session = requests.Session()
        if self.auth:
            self.session.auth = self.auth
        # For simplicity, disable SSL verification (adjust as needed)
        self.session.verify = False
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.logger = logging.getLogger("GenericAPIClient")

    def make_request(self, endpoint: str, params: dict = None, retries: int = 3, base_delay: float = 1.0):
        full_url = f"{self.base_url}{endpoint}"
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {full_url} (Attempt {attempt+1}/{retries}), params={params}")
                response = self.session.get(full_url, params=params)
                response.raise_for_status()
                return response.json(), None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                status_code = response.status_code if response else "N/A"
                self.logger.error(
                    f"Error calling {full_url}: {error_message} (Status: {status_code})",
                    extra={"error_category": "API Failure", "severity": "error"}
                )
                sleep_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                time.sleep(sleep_time)
        return None, "Max retries exceeded"

# ---------------------------------------------------------------------
# Get Raw File Content from Bitbucket
# ---------------------------------------------------------------------
def get_file_content(api_client: GenericAPIClient, project_key: str, repo_slug: str, branch: str, file_path: str) -> str:
    """
    Fetch raw file content.
    Endpoint: /projects/{project_key}/repos/{repo_slug}/raw/{file_path}?at={branch}
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/raw/{file_path}"
    params = {"at": branch}
    api_client.token_bucket.consume(1)
    try:
        url = f"{api_client.base_url}{endpoint}"
        response = api_client.session.get(url, params=params)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logger.error(f"Failed to fetch content for {file_path} in {project_key}/{repo_slug}: {e}",
                     extra={"error_category": "File Fetch Error", "severity": "error"})
        return ""

# ---------------------------------------------------------------------
# Helper Function: Get Top (Most Recently Modified) Branch
# ---------------------------------------------------------------------
def get_top_branch(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> str:
    """
    Retrieve the top-most branch (ordered by modification date) for the given repository.
    If unable to retrieve, fall back to 'master'.
    """
    endpoint = f"/projects/{project_key}/repos/{repo_slug}/branches"
    params = {"orderBy": "MODIFICATION", "limit": 1}
    data, error = api_client.make_request(endpoint, params=params)
    if error or not data or "values" not in data or not data["values"]:
        logger.warning(f"Unable to get top branch for {project_key}/{repo_slug}: Using fallback 'master'")
        return "master"
    return data["values"][0].get("displayId", "master")

# ---------------------------------------------------------------------
# Extension and Filename Mappings for Fallback
# ---------------------------------------------------------------------
EXT_LANG_MAP = {
    ".py": "Python",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
    ".rb": "Ruby",
    ".cs": "C#",
    ".cpp": "C++",
    ".c": "C",
    ".go": "Go",
    ".php": "PHP",
    ".rs": "Rust",
    ".kt": "Kotlin",
    ".swift": "Swift",
    # Extend if needed.
}

FILE_FRAMEWORK_MAP = {
    "pom.xml": "Maven",
    "build.gradle": "Gradle",
    "package.json": "Node.js / npm",
    "requirements.txt": "Python (pip)",
    "Gemfile": "Ruby on Rails",
    "composer.json": "Composer (PHP)",
    "Cargo.toml": "Rust (Cargo)",
    "Makefile": "Make",
    # Extend if needed.
}

# ---------------------------------------------------------------------
# API Calls to Retrieve Repositories and Files
# ---------------------------------------------------------------------
def get_repos(api_client: GenericAPIClient, project_key: str) -> list:
    """Retrieve all repositories for the given project (SPK)."""
    endpoint = f"/projects/{project_key}/repos"
    params = {"limit": 1000}
    data, error = api_client.make_request(endpoint, params=params)
    if error:
        logger.error(
            f"Failed to get repos for project {project_key}: {error}",
            extra={"error_category": "API Failure", "severity": "error"}
        )
        return []
    return data.get("values", [])

def get_files_with_pagination(api_client: GenericAPIClient, project_key: str, repo_slug: str) -> list:
    """
    Retrieve all files in the repository using pagination (limit=1000).
    Stops if 'nextPageStart' is missing or None.
    """
    all_files = []
    start = 0
    limit = 1000
    while True:
        endpoint = f"/projects/{project_key}/repos/{repo_slug}/files"
        params = {"limit": limit, "start": start}
        response_data, error = api_client.make_request(endpoint, params=params)
        if error:
            logger.error(
                f"Failed to retrieve files for {project_key}/{repo_slug} (start={start}): {error}",
                extra={"error_category": "API Failure", "severity": "error"}
            )
            break
        file_values = response_data.get("values", [])
        all_files.extend(file_values)
        next_page = response_data.get("nextPageStart")
        if not next_page:
            break
        start = next_page
    return all_files

# ---------------------------------------------------------------------
# Analysis Logic: Using Lexer or Fallback Based on File Extension
# ---------------------------------------------------------------------
def analyze_file_list(file_list: list, use_lexer: bool, api_client=None, project_key=None, repo_slug=None, branch="master") -> dict:
    """
    Analyze the file list.
      1. Collate all unique extensions with a sample file.
      2. If use_lexer is enabled, fetch a sample file's content and use Pygments to guess the language.
         Otherwise, use a simple extension mapping.
      3. Apply heuristics on folder names and specific file names to detect frameworks.
    """
    unique_extensions = {}  # Map extension -> sample file path.
    for file_path in file_list:
        base_name = os.path.basename(file_path)
        ext = os.path.splitext(base_name)[1].lower()
        if ext and ext not in unique_extensions:
            unique_extensions[ext] = file_path

    languages = set()
    frameworks = set()

    if use_lexer and api_client and project_key and repo_slug:
        for ext, sample_file in unique_extensions.items():
            content = get_file_content(api_client, project_key, repo_slug, branch, sample_file)
            if content:
                try:
                    lexer = guess_lexer_for_filename("dummy" + ext, content)
                    languages.add(lexer.name)
                except ClassNotFound:
                    if ext in EXT_LANG_MAP:
                        languages.add(EXT_LANG_MAP[ext])
                except Exception as e:
                    logger.error(f"Lexer error for file {sample_file}: {e}",
                                 extra={"error_category": "Lexer Error", "severity": "error"})
                    if ext in EXT_LANG_MAP:
                        languages.add(EXT_LANG_MAP[ext])
            else:
                if ext in EXT_LANG_MAP:
                    languages.add(EXT_LANG_MAP[ext])
    else:
        for ext in unique_extensions.keys():
            if ext in EXT_LANG_MAP:
                languages.add(EXT_LANG_MAP[ext])

    # Apply additional heuristics based on file names and folder structure.
    for file_path in file_list:
        base_name = os.path.basename(file_path)
        if base_name in FILE_FRAMEWORK_MAP:
            frameworks.add(FILE_FRAMEWORK_MAP[base_name])
        if "node_modules" in file_path:
            frameworks.add("Node.js")
        if "venv" in file_path or "env" in file_path:
            frameworks.add("Python VirtualEnv")
        if "src/main/java" in file_path:
            languages.add("Java")
        if "src/main/resources" in file_path:
            frameworks.add("Spring")

    return {
        "unique_extensions": list(sorted(unique_extensions.keys())),
        "languages": list(sorted(languages)),
        "frameworks": list(sorted(frameworks)),
        "file_count": len(file_list)
    }

# ---------------------------------------------------------------------
# Repo Output: Write Individual Excel File per Processed Repository
# ---------------------------------------------------------------------
def write_repo_output(project_key: str, repo_slug: str, record: dict):
    """
    Write the analysis record to an Excel file named after the project key and repo slug.
    For example: C:\riskportal\PROJECT_REPO.xlsx
    """
    output_filename = f"{project_key}_{repo_slug}.xlsx"
    output_filepath = os.path.join(OUTPUT_FOLDER, output_filename)
    df = pd.DataFrame([record])
    df.to_excel(output_filepath, index=False)
    logger.info(f"Wrote output for {project_key}/{repo_slug} to {output_filepath}")

def already_processed(project_key: str, repo_slug: str) -> bool:
    """
    Check if the output file for this repo already exists.
    """
    output_filename = f"{project_key}_{repo_slug}.xlsx"
    output_filepath = os.path.join(OUTPUT_FOLDER, output_filename)
    return os.path.isfile(output_filepath)

# ---------------------------------------------------------------------
# Process a Single Repository
# ---------------------------------------------------------------------
def process_single_repo(api_client: GenericAPIClient, project_key: str, repo: dict) -> None:
    repo_slug = repo.get("slug")
    if already_processed(project_key, repo_slug):
        logger.info(f"Skipping {project_key}/{repo_slug} as output file already exists.")
        return

    # Always use the top-most branch instead of the default.
    top_branch = get_top_branch(api_client, project_key, repo_slug)

    file_paths = get_files_with_pagination(api_client, project_key, repo_slug)
    analysis = analyze_file_list(file_paths, USE_LEXER_ANALYSIS, api_client, project_key, repo_slug, top_branch)

    repo_id = repo.get("id")
    repo_name = repo.get("name", repo_slug)
    links = repo.get("links", {})
    self_links = links.get("self", [])
    if self_links and isinstance(self_links, list):
        repo_url = self_links[0].get("href", f"{api_client.base_url}/projects/{project_key}/repos/{repo_slug}")
    else:
        repo_url = f"{api_client.base_url}/projects/{project_key}/repos/{repo_slug}"

    record = {
        "project_key": project_key,
        "repo_slug": repo_slug,
        "repo_id": repo_id,
        "repo_url": repo_url,
        "repo_name": repo_name,
        "file_count": analysis["file_count"],
        "unique_extensions": ", ".join(analysis["unique_extensions"]),
        "languages": ", ".join(analysis["languages"]),
        "frameworks": ", ".join(analysis["frameworks"])
    }
    logger.info(f"Processed {project_key}/{repo_slug} -> {record}")
    write_repo_output(project_key, repo_slug, record)

# ---------------------------------------------------------------------
# Process a Single SPK (Project)
# ---------------------------------------------------------------------
def process_single_spk(api_client: GenericAPIClient, project_key: str, max_repos: int) -> None:
    repos = get_repos(api_client, project_key)
    if max_repos is not None:
        repos = repos[:max_repos]
    for repo in repos:
        process_single_repo(api_client, project_key, repo)

# ---------------------------------------------------------------------
# Main Orchestration
# ---------------------------------------------------------------------
def main():
    start_time = datetime.now()
    logger.info("BitbucketRepoAnalyzer started.")

    if not os.path.isfile(INPUT_EXCEL):
        logger.error(f"Input file not found: {INPUT_EXCEL}")
        return

    wb = load_workbook(INPUT_EXCEL)
    sheet = wb.active
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    spk_list = [row[0] for row in spk_rows if row and row[0]]

    token_bucket = TokenBucket(TOKEN_BUCKET_CAPACITY, TOKENS_PER_SECOND)
    api_client = GenericAPIClient(BITBUCKET_BASE_URL, token_bucket, auth=(BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD))

    def spk_worker(spk: str) -> str:
        process_single_spk(api_client, spk, MAX_REPOS_PER_SPK)
        return spk

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(spk_worker, spk): spk for spk in spk_list}
        for future in as_completed(futures):
            spk_key = futures[future]
            try:
                future.result()
                logger.info(f"Finished processing SPK {spk_key}")
            except Exception as e:
                logger.error(f"Error processing SPK {spk_key}: {e}",
                             extra={"error_category": "Processing Error", "severity": "error"})

    end_time = datetime.now()
    logger.info(f"BitbucketRepoAnalyzer completed in {end_time - start_time}")

if __name__ == "__main__":
    main()
