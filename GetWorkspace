import pandas as pd
import requests
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# Configure logging
logging.basicConfig(filename='processCommitIDs/bitbucket_audit.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Bitbucket API Credentials
BITBUCKET_USERNAME = "nbkpjqb"
BITBUCKET_PASSWORD = "your_password"
BITBUCKET_BASE_URL = "https://scm.horizon.bankofamerica.com/rest/api/latest"

# Rate Limit Settings
MAX_THREADS = 3  # Limit concurrency
RETRY_LIMIT = 3  # Retry up to 3 times
INITIAL_BACKOFF = 10  # Initial wait time in seconds

# Date Range for Commits
START_DATE = "2024-01-01T00:00:00Z"
END_DATE = datetime.utcnow().isoformat() + "Z"

# Read SPK.xlsx (project keys)
def get_project_keys(spk_file):
    df = pd.read_excel(spk_file)
    df.columns = df.columns.str.lower()  # Handle case insensitivity
    if 'spk' not in df.columns:
        raise ValueError("SPK.xlsx must contain a 'SPK' column.")
    return df['spk'].dropna().unique().tolist()

# Read commit.xlsx
def get_commit_data(commit_file):
    df = pd.read_excel(commit_file)
    df.columns = df.columns.str.lower()  # Handle case insensitivity
    required_columns = {'repository_name', 'branch_name', 'commit_id', 'spk'}
    if not required_columns.issubset(df.columns):
        raise ValueError(f"Missing required columns: {required_columns - set(df.columns)}")
    df['updated_spk'] = ''
    return df

# Bitbucket API Request with retries
def bitbucket_api_request(url):
    headers = {"Accept": "application/json"}
    retries = 0
    backoff = INITIAL_BACKOFF
    while retries < RETRY_LIMIT:
        response = requests.get(url, auth=(BITBUCKET_USERNAME, BITBUCKET_PASSWORD), headers=headers, verify=False)
        
        if response.status_code == 200:
            return response.json()
        elif response.status_code == 429:  # Rate Limit Hit
            retry_after = int(response.headers.get("Retry-After", backoff))
            logging.warning(f"Rate limit hit. Retrying after {retry_after} seconds.")
            time.sleep(retry_after)
        elif response.status_code >= 500:
            logging.warning(f"Server error {response.status_code}. Retrying {retries + 1}/{RETRY_LIMIT}...")
            time.sleep(backoff)
        else:
            logging.error(f"Failed API call to {url}: {response.status_code} - {response.text}")
            return None
        
        retries += 1
        backoff *= 2  # Exponential backoff
    return None

# Get all repositories in a project (SPK)
def get_repositories(project_key):
    url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos?limit=100"
    repos = []
    while url:
        data = bitbucket_api_request(url)
        if data:
            repos.extend([repo['slug'] for repo in data.get('values', [])])
            url = data.get('next')  # Handle pagination
        else:
            break
    return repos

# Get all commits in a repository within the date range
def get_all_commits(project_key, repo):
    url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos/{repo}/commits?limit=100&since={START_DATE}&until={END_DATE}"
    commits = set()
    while url:
        data = bitbucket_api_request(url)
        if data:
            commits.update(commit['id'] for commit in data.get('values', []))
            url = data.get('next')
        else:
            break
    return commits

# Process each project
def process_project(project_key, commit_df):
    logging.info(f"Processing project: {project_key}")
    updated_rows = 0
    
    repos = get_repositories(project_key)
    for repo in repos:
        commits = get_all_commits(project_key, repo)
        
        for index, row in commit_df.iterrows():
            if row['commit_id'] in commits:
                commit_df.at[index, 'updated_spk'] = project_key
                updated_rows += 1
    
    logging.info(f"Updated {updated_rows} rows for project: {project_key}")
    return commit_df

# Main execution
def update_commit_file(commit_file, spk_file):
    commit_df = get_commit_data(commit_file)
    project_keys = get_project_keys(spk_file)
    
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = {executor.submit(process_project, pk, commit_df.copy()): pk for pk in project_keys}
        for future in as_completed(futures):
            pk = futures[future]
            try:
                result_df = future.result()
                commit_df.update(result_df)
            except Exception as e:
                logging.error(f"Error processing project {pk}: {e}")
    
    # Save updated file
    output_file = "processCommitIDs/updated_commit.xlsx"
    commit_df.to_excel(output_file, index=False)
    logging.info(f"Updated file saved as {output_file}")
    return output_file

# Example usage
# update_commit_file('processCommitIDs/commit.xlsx', 'processCommitIDs/SPK.xlsx')
