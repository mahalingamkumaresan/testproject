#
import re
import threading
import logging
import json
import time
import os
import glob
import pandas as pd
import requests
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from openpyxl import load_workbook
from requests.adapters import HTTPAdapter

# ------------------------------
# Custom JSON Logging Formatter
# ------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

# Setup logging: logs to console and to a file in the output folder.
def configure_logging(output_folder):
    output_log_path = os.path.join(output_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(output_log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    return logging.getLogger("Main")

main_logger = None  # Will be set in main().

# ------------------------------
# Utility: Read Already-Processed Commit IDs from Existing Parquet Files
# ------------------------------
def load_processed_commit_ids(output_folder):
    processed_commits = set()
    for file in glob.glob(os.path.join(output_folder, "commit_output_part_*.parquet")):
        try:
            df_existing = pd.read_parquet(file, engine="pyarrow")
            # Check for either column name; we assume commit IDs are in column "COMMIT_ID" or "Commit Id"
            if "COMMIT_ID" in df_existing.columns:
                processed_commits.update(df_existing["COMMIT_ID"].unique())
            elif "Commit Id" in df_existing.columns:
                processed_commits.update(df_existing["Commit Id"].unique())
        except Exception as e:
            main_logger.error(f"Error reading {file}: {e}")
    return processed_commits

# ------------------------------
# Token Bucket for Rate Limiting
# ------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float):
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1):
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ------------------------------
# Bitbucket API Wrapper with Connection Pool Increase
# ------------------------------
class BitbucketAPI:
    def __init__(self, base_url: str, username: str, app_password: str, token_bucket: TokenBucket):
        self.base_url = base_url
        self.username = username
        self.app_password = app_password
        self.token_bucket = token_bucket
        self.session = requests.Session()
        self.session.auth = (self.username, self.app_password)
        self.session.verify = False
        adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        self.logger = logging.getLogger("BitbucketAPI")

    def make_request(self, url: str, retries: int = 2, delay: int = 2):
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {url} (Attempt {attempt+1}/{retries})")
                response = self.session.get(url)
                response.raise_for_status()
                return response, None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                status_code = response.status_code if response is not None else "N/A"
                if response is not None and response.status_code == 429:
                    self.logger.critical(f"429 Too Many Requests encountered for {url}!", 
                                           extra={"error_category": "Rate Limit", "severity": "critical"})
                extra = {"error_category": "API Failure", "severity": "error"}
                self.logger.error(f"Request failed for {url}: {error_message} (Status: {status_code})", extra=extra)
                if response is not None and response.status_code in [404, 401]:
                    try:
                        error_message = response.json().get('errors', [{}])[0].get('message', error_message)
                        return None, error_message
                    except Exception:
                        pass
                if attempt < retries - 1:
                    time.sleep(delay)
                if response is not None and response.status_code == 429:
                    time.sleep(delay * 10)
                else:
                    return None, error_message
        return None, "Max retries exceeded"

# ------------------------------
# Helper Function: Diff Parsing from API Response
# ------------------------------
def parse_diff_change(change: dict) -> (int, int, int):
    lines_added = 0
    lines_removed = 0
    lines_modified = 0  # Not provided separately by the API
    try:
        for hunk in change.get('hunks', []):
            for segment in hunk.get('segments', []):
                if segment.get('type') == 'REMOVED':
                    for _ in segment.get('lines', []):
                        lines_removed += 1
                elif segment.get('type') == 'ADDED':
                    for _ in segment.get('lines', []):
                        lines_added += 1
    except Exception as e:
        logging.getLogger("DiffParser").error(f"Error parsing diff change: {e}")
        raise e
    return lines_added, lines_removed, lines_modified

# ------------------------------
# Commit Processor: API-Only for SPK Mode (Commit Collector)
# ------------------------------
class CommitProcessor:
    def __init__(self, api: BitbucketAPI, output_folder: str, author_email_filter: set = None, recent_branches: int = 5):
        self.api = api
        self.output_folder = output_folder
        self.author_email_filter = author_email_filter
        self.recent_branches = recent_branches
        self.logger = logging.getLogger("CommitProcessor")
        self.failed_urls = []
        self.diff_failures = []

    def get_top_branches(self, project_key: str, repo_slug: str, start_date: str, end_date: str):
        branch_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/branches?limit=1000"
        response, err = self.api.make_request(branch_url)
        if not response:
            self.failed_urls.append({
                "url": branch_url,
                "reason": err,
                "error_category": "API Failure",
                "severity": "error"
            })
            return []
        branches_data = response.json().get("values", [])
        branches_with_timestamp = []
        for branch in branches_data:
            branch_name = branch.get("displayId")
            latest_commit = branch.get("latestCommit")
            if not branch_name or not latest_commit:
                continue
            commit_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{latest_commit}"
            commit_resp, commit_err = self.api.make_request(commit_url)
            if not commit_resp:
                self.failed_urls.append({
                    "url": commit_url,
                    "reason": commit_err,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                continue
            commit_details = commit_resp.json()
            timestamp = commit_details.get("authorTimestamp")
            if timestamp:
                branches_with_timestamp.append({
                    "branch": branch_name,
                    "timestamp": timestamp
                })
        top_branches = sorted(branches_with_timestamp, key=lambda x: x["timestamp"], reverse=True)[:self.recent_branches]
        result = [b["branch"] for b in top_branches]
        self.logger.debug(f"Top {self.recent_branches} branches for {project_key}/{repo_slug}: {result}")
        return result

    def process_commits_spk(self, project_key: str, repo_slug: str, start_date: str, end_date: str):
        top_branches = self.get_top_branches(project_key, repo_slug, start_date, end_date)
        deduped_commits = {}
        for branch in top_branches:
            url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits?limit=1000&until={branch}"
            response, error_reason = self.api.make_request(url)
            if not response:
                self.failed_urls.append({
                    "url": url,
                    "reason": error_reason,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                continue
            data = response.json()
            for commit in data.get('values', []):
                commit_id = commit.get('id', '')
                if commit_id not in deduped_commits:
                    deduped_commits[commit_id] = commit
        output_rows = []
        for commit in deduped_commits.values():
            commit_date = datetime.fromtimestamp(commit["authorTimestamp"] / 1000)
            commit_month = commit_date.strftime('%Y-%m')
            author_name = (commit["author"].get('displayName') or 
                           commit["author"].get('displayname') or 
                           commit["author"].get('name') or 'Unknown Author')
            author_email = (commit["author"].get('emailAddress') or 
                            commit["author"].get('emailaddress') or '').lower()
            if self.author_email_filter and author_email not in self.author_email_filter:
                continue
            commit_id = commit.get('id', '')
            row = {
                "Project Key": project_key,
                "Repo Slug": repo_slug,
                "Author Name": author_name,
                "Author Email": author_email,
                "Commit Month": commit_month,
                "Commit Id": commit_id,
                "Commit Message": commit.get("message", "")
            }
            output_rows.append(row)
        return output_rows

    def process_commits_by_ids(self, commit_id_file: str):
        # Read commit_ids.xlsx; expect columns: COMMIT_ID, REPOSITORY_NAME, SPK, EMAIL_ID, FULL_NAME.
        df = pd.read_excel(commit_id_file)
        # Read existing commit IDs from previously produced output files.
        existing_commits = load_processed_commit_ids(self.output_folder)
        df = df[~df["COMMIT_ID"].isin(existing_commits)]
        output_rows = []
        for index, row in df.iterrows():
            commit_id = row["COMMIT_ID"]
            project_key = row["SPK"]
            repo_slug = row["REPOSITORY_NAME"]
            self.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug} (API mode)")
            diff_url = f"{self.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
            file_response, diff_error = self.api.make_request(diff_url)
            if not file_response:
                self.failed_urls.append({
                    "url": diff_url,
                    "reason": diff_error,
                    "error_category": "API Failure",
                    "severity": "error"
                })
                continue
            try:
                diff_data = file_response.json()
            except Exception as e:
                self.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": str(e),
                    "error_category": "Diff Parsing Failure",
                    "severity": "error"
                })
                continue
            commit_month = "Unknown"  # Timestamp not available in commit_ids.xlsx
            first_row = True
            if 'diffs' not in diff_data or not diff_data['diffs']:
                self.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": "No diff data found",
                    "error_category": "Diff Parsing Failure",
                    "severity": "warning"
                })
                continue
            author_name = row.get("FULL_NAME", "Unknown")
            author_email = row.get("EMAIL_ID", "Unknown").lower()
            for change in diff_data.get("diffs", []):
                source_info = change.get('source', {})
                dest_info = change.get('destination', {})
                file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                             else source_info.get('name', 'Unknown'))
                file_status = "modified"
                if not source_info:
                    file_status = "new"
                elif not dest_info:
                    file_status = "deleted"
                try:
                    added, removed, modified = parse_diff_change(change)
                    if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                        self.diff_failures.append({
                            "url": diff_url,
                            "commit_id": commit_id,
                            "reason": "No changes found in diff",
                            "error_category": "Diff Parsing Failure",
                            "severity": "warning"
                        })
                except Exception as e:
                    self.diff_failures.append({
                        "url": diff_url,
                        "commit_id": commit_id,
                        "reason": f"Exception during diff parsing: {e}",
                        "error_category": "Diff Parsing Failure",
                        "severity": "error"
                    })
                    continue
                out_row = {
                    "Project Key": project_key,
                    "Repo Slug": repo_slug,
                    "Author Name": author_name,
                    "Author Email": author_email,
                    "Commit Month": commit_month,
                    "File Name": file_name,
                    "Month Lines Added": added,
                    "Month Lines Removed": removed,
                    "Month Lines Modified": modified,
                    "Total Commits": 1 if first_row else 0,
                    "Commit Id": commit_id,
                    "File Status": file_status
                }
                output_rows.append(out_row)
                first_row = False
        return output_rows

    def write_chunk(self, rows: list, chunk_index: int, filename_prefix=None):
        if rows:
            df = pd.DataFrame(rows)
            if filename_prefix:
                output_path = os.path.join(self.output_folder, f"{filename_prefix}.parquet")
            else:
                output_path = os.path.join(self.output_folder, f"commit_file_output_part_{chunk_index}.parquet")
            df.to_parquet(output_path, engine="pyarrow", index=False)
            self.logger.info(f"Output written with {len(rows)} rows to {output_path}")
        else:
            self.logger.info(f"No rows to write for chunk {chunk_index}.")

    def write_failure_files(self):
        if self.failed_urls:
            df_failed = pd.DataFrame(self.failed_urls)
            failed_path = os.path.join(self.output_folder, "failed_urls.parquet")
            df_failed.to_parquet(failed_path, engine="pyarrow", index=False)
            self.logger.info(f"Failed URLs written to {failed_path}")
        if self.diff_failures:
            df_diff_fail = pd.DataFrame(self.diff_failures)
            diff_fail_path = os.path.join(self.output_folder, "diff_failures.parquet")
            df_diff_fail.to_parquet(diff_fail_path, engine="pyarrow", index=False)
            self.logger.info(f"Diff failures written to {diff_fail_path}")

# ------------------------------
# Main Processing Functions for SPK Mode (Commit Collector)
# ------------------------------
def process_spk_mode(input_folder: str, output_folder: str, start_date: str, end_date: str, recent_branches: int, order: str):
    project_keys_file = os.path.join(input_folder, 'WTMSPK.xlsx')
    filter_file = os.path.join(input_folder, 'author_filter.xlsx')
    wb = load_workbook(project_keys_file)
    sheet = wb.active
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    total_spks = len(spk_rows)
    if order == "bottom-up":
        spk_rows.reverse()
    spk_progress = 0

    filter_df = pd.read_excel(filter_file)
    author_email_filter = set(filter_df['Author Email'].str.lower())

    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    token_bucket = TokenBucket(capacity=70, tokens_per_second=1)
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    processor = CommitProcessor(api, output_folder, author_email_filter, recent_branches=recent_branches)

    successful_repos = 0
    total_repos = 0
    futures = {}
    with ThreadPoolExecutor(max_workers=5) as executor:
        for row in spk_rows:
            project_key = row[0]
            if not project_key:
                continue
            futures[project_key] = executor.submit(process_project, project_key, processor, start_date, end_date)
        for future in as_completed(futures):
            spk_progress += 1
            proj_key, repo_outputs = future.result()  # repo_outputs: list of (repo_slug, output_rows)
            for repo_slug, rows in repo_outputs:
                total_repos += 1
                if rows:
                    filename = f"output_{proj_key}_{repo_slug}"
                    processor.write_chunk(rows, 1, filename_prefix=filename)
                    main_logger.info(f"Output file for project {proj_key} repo {repo_slug}: {filename}.parquet")
                    successful_repos += 1
                else:
                    main_logger.info(f"No commits collected for project {proj_key} repo {repo_slug}.")
            main_logger.info(f"Finished processing project {proj_key} ({spk_progress}/{total_spks})")
    processor.write_failure_files()
    main_logger.info(f"Successfully collected commits for {successful_repos} out of {total_repos} repos.")
    main_logger.info(f"Total tokens consumed: {token_bucket.calls_made}")
    main_logger.info(f"Remaining token capacity: {token_bucket.tokens}")

def process_project(project_key: str, processor: CommitProcessor, start_date: str, end_date: str):
    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    api = processor.api
    repo_outputs = []  # List of tuples: (repo_slug, output_rows)
    repos_url = f"{BITBUCKET_BASE_URL}/projects/{project_key}/repos?limit=1000"
    response, error_reason = api.make_request(repos_url)
    if not response:
        processor.failed_urls.append({
            "url": repos_url,
            "reason": error_reason,
            "error_category": "API Failure",
            "severity": "error"
        })
        return (project_key, repo_outputs)
    repos_data = response.json()
    for repo in repos_data.get('values', []):
        repo_slug = repo.get('slug')
        if not repo_slug:
            continue
        rows = processor.process_commits_spk(project_key, repo_slug, start_date, end_date)
        repo_outputs.append((repo_slug, rows))
    return (project_key, repo_outputs)

# ------------------------------
# Main Processing Functions for Commit Mode (API-Only)
# ------------------------------
def process_commit_mode(input_folder: str, output_folder: str, chunk_size: int = 10000):
    commit_id_file = os.path.join(input_folder, 'commit_ids.xlsx')
    # Load commit_ids.xlsx
    df = pd.read_excel(commit_id_file)
    # Read existing commit IDs from existing output files.
    processed_commits = load_processed_commit_ids(output_folder)
    df = df[~df["COMMIT_ID"].isin(processed_commits)]
    total_commits = len(df)
    main_logger.info(f"Total commit IDs to process: {total_commits}")
    
    token_bucket = TokenBucket(capacity=70, tokens_per_second=1)
    BITBUCKET_BASE_URL = os.getenv('BITBUCKET_BASE_URL', 'https://scm.horizon.bankofamerica.com/rest/api/latest')
    BITBUCKET_USERNAME = os.getenv('BITBUCKET_USERNAME', '')
    BITBUCKET_APP_PASSWORD = os.getenv('BITBUCKET_APP_PASSWORD', '')
    api = BitbucketAPI(BITBUCKET_BASE_URL, BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD, token_bucket)
    processor = CommitProcessor(api, output_folder)
    
    def process_chunk(chunk_df, chunk_index):
        rows = process_commit_chunk(processor, chunk_df)
        filename = f"commit_output_part_{chunk_index}"
        processor.write_chunk(rows, chunk_index, filename_prefix=filename)
        processed_count = min(chunk_index * chunk_size, total_commits)
        main_logger.info(f"Processed {processed_count} out of {total_commits} commit IDs (Chunk {chunk_index}: Output file: {filename}.parquet)")
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for i in range(0, total_commits, chunk_size):
            chunk_df = df.iloc[i:i+chunk_size]
            futures.append(executor.submit(process_chunk, chunk_df, i//chunk_size + 1))
        for future in as_completed(futures):
            future.result()
    
    processor.write_failure_files()
    main_logger.info(f"Total tokens consumed: {token_bucket.calls_made}")
    main_logger.info(f"Remaining token capacity: {token_bucket.tokens}")

def process_commit_chunk(processor: 'CommitProcessor', chunk_df: pd.DataFrame):
    output_rows = []
    for index, row in chunk_df.iterrows():
        commit_id = row["COMMIT_ID"]
        project_key = row["SPK"]
        repo_slug = row["REPOSITORY_NAME"]
        processor.logger.debug(f"Processing commit {commit_id} in {project_key}/{repo_slug} (API mode)")
        diff_url = f"{processor.api.base_url}/projects/{project_key}/repos/{repo_slug}/commits/{commit_id}/diff?ignore_whitespace=true"
        file_response, diff_error = processor.api.make_request(diff_url)
        if not file_response:
            processor.failed_urls.append({
                "url": diff_url,
                "reason": diff_error,
                "error_category": "API Failure",
                "severity": "error"
            })
            continue
        try:
            diff_data = file_response.json()
        except Exception as e:
            processor.diff_failures.append({
                "url": diff_url,
                "commit_id": commit_id,
                "reason": str(e),
                "error_category": "Diff Parsing Failure",
                "severity": "error"
            })
            continue
        commit_month = "Unknown"  # Timestamp not provided in commit_ids.xlsx
        first_row = True
        if 'diffs' not in diff_data or not diff_data['diffs']:
            processor.diff_failures.append({
                "url": diff_url,
                "commit_id": commit_id,
                "reason": "No diff data found",
                "error_category": "Diff Parsing Failure",
                "severity": "warning"
            })
            continue
        author_name = row.get("FULL_NAME", "Unknown")
        author_email = row.get("EMAIL_ID", "Unknown").lower()
        for change in diff_data.get("diffs", []):
            source_info = change.get('source', {})
            dest_info = change.get('destination', {})
            file_name = (dest_info.get('name') if dest_info and dest_info.get('name')
                         else source_info.get('name', 'Unknown'))
            file_status = "modified"
            if not source_info:
                file_status = "new"
            elif not dest_info:
                file_status = "deleted"
            try:
                added, removed, modified = parse_diff_change(change)
                if added == 0 and removed == 0 and modified == 0 and file_status == "modified":
                    processor.diff_failures.append({
                        "url": diff_url,
                        "commit_id": commit_id,
                        "reason": "No changes found in diff",
                        "error_category": "Diff Parsing Failure",
                        "severity": "warning"
                    })
            except Exception as e:
                processor.diff_failures.append({
                    "url": diff_url,
                    "commit_id": commit_id,
                    "reason": f"Exception during diff parsing: {e}",
                    "error_category": "Diff Parsing Failure",
                    "severity": "error"
                })
                continue
            out_row = {
                "Project Key": project_key,
                "Repo Slug": repo_slug,
                "Author Name": author_name,
                "Author Email": author_email,
                "Commit Month": commit_month,
                "File Name": file_name,
                "Month Lines Added": added,
                "Month Lines Removed": removed,
                "Month Lines Modified": modified,
                "Total Commits": 1 if first_row else 0,
                "Commit Id": commit_id,
                "File Status": file_status
            }
            output_rows.append(out_row)
            first_row = False
    return output_rows

# ------------------------------
# Main Entry Point with Argument Parsing
# ------------------------------
def main():
    parser = argparse.ArgumentParser(description="Process Bitbucket commits (API-only).")
    parser.add_argument('--mode', choices=['spk', 'commit'], default='commit',
                        help="Input mode: 'spk' for SPK list mode, 'commit' for commit id mode. (Default: commit)")
    parser.add_argument('--start_date', default='2024-01-01',
                        help="Start date (YYYY-MM-DD) for commit filtering (used in SPK mode).")
    parser.add_argument('--end_date', default='2025-03-31',
                        help="End date (YYYY-MM-DD) for commit filtering (used in SPK mode).")
    parser.add_argument('--recent_branches', type=int, default=5,
                        help="Number of recent branches to process in SPK mode (default: 5).")
    parser.add_argument('--order', choices=['top-down', 'bottom-up'], default='top-down',
                        help="Processing order for SPKs (top-down or bottom-up).")
    parser.add_argument('--commit_chunk_size', type=int, default=10000,
                        help="Chunk size for processing commit mode (default: 10000).")
    args = parser.parse_args()

    common_folder = "C:/Riskportal/3Mar2025_JavaTrainings"
    input_folder = os.path.join(common_folder, "input")
    output_folder = os.path.join(common_folder, "output")
    
    # Configure logging in the output folder.
    for handler in logging.getLogger().handlers[:]:
        logging.getLogger().removeHandler(handler)
    output_log_path = os.path.join(output_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(output_log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    global main_logger
    main_logger = logging.getLogger("Main")

    if args.mode == 'spk':
        main_logger.info("Running in SPK mode (API-only, no pagination, top 5 branches per repo).")
        process_spk_mode(input_folder, output_folder, args.start_date, args.end_date, args.recent_branches, args.order)
    elif args.mode == 'commit':
        main_logger.info("Running in Commit Id mode.")
        process_commit_mode(input_folder, output_folder, args.commit_chunk_size)

if __name__ == "__main__":
    main()
