#!/usr/bin/env python3
"""
MinimalRepoFileAnalyzer

Purpose:
  - Reads SPKs (project keys) from an Excel file (e.g., WTMSPK.xlsx).
  - For each SPK:
      1. Retrieves the list of repositories (with IDs and URL information).
      2. Processes up to a configurable maximum number of repositories per SPK.
      3. For each repository, fetches all files (via pagination) in the default branch.
      4. Analyzes file paths to infer:
           - Programming languages (by file extensions)
           - Frameworks/technologies (by known filenames/folder structure)
           - Unique file extensions present in the repository
           - Total file count.
  - Writes the final analysis results to an Excel file.
"""

import os
import time
import json
import random
import logging
import threading
import requests
import openpyxl
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# ------------------------------
# Configuration Variables
# ------------------------------
# Set the maximum number of repositories to process per SPK.
MAX_REPOS_PER_SPK = 10  # Set to None to process all repos if desired.

# ------------------------------
# Custom JSON Logging Formatter
# ------------------------------
class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "module": record.name,
            "message": record.getMessage()
        }
        if hasattr(record, "error_category"):
            log_record["error_category"] = record.error_category
        if hasattr(record, "severity"):
            log_record["severity"] = record.severity
        return json.dumps(log_record)

def configure_logging(logs_folder: str) -> logging.Logger:
    os.makedirs(logs_folder, exist_ok=True)
    log_path = os.path.join(logs_folder, "process_log.txt")
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JsonFormatter())
    file_handler = logging.FileHandler(log_path)
    file_handler.setFormatter(JsonFormatter())
    logging.basicConfig(level=logging.DEBUG, handlers=[console_handler, file_handler])
    return logging.getLogger("MinimalRepoFileAnalyzer")

logger = configure_logging("logs")

# ------------------------------
# Token Bucket for Rate Limiting
# ------------------------------
class TokenBucket:
    def __init__(self, capacity: int, tokens_per_second: float) -> None:
        self.capacity = capacity
        self.tokens_per_second = tokens_per_second
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
        self.calls_made = 0

    def consume(self, tokens: int = 1) -> None:
        while True:
            with self.lock:
                now = time.time()
                elapsed = now - self.last_refill
                refill = elapsed * self.tokens_per_second
                if refill > 0:
                    self.tokens = min(self.capacity, self.tokens + refill)
                    self.last_refill = now
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    self.calls_made += tokens
                    return
            time.sleep(0.1)

# ------------------------------
# Generic API Client with Exponential Backoff
# ------------------------------
class GenericAPIClient:
    def __init__(self, base_url: str, token_bucket: TokenBucket, auth: tuple = None):
        self.base_url = base_url.rstrip("/")
        self.token_bucket = token_bucket
        self.auth = auth
        self.session = requests.Session()
        if self.auth:
            self.session.auth = self.auth
        # Adjust SSL verification and adapter settings as needed.
        self.session.verify = False
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.logger = logging.getLogger("GenericAPIClient")

    def make_request(self, endpoint: str, params: dict = None, retries: int = 3, base_delay: float = 1.0):
        full_url = f"{self.base_url}{endpoint}"
        response = None
        for attempt in range(retries):
            self.token_bucket.consume(1)
            try:
                self.logger.debug(f"Requesting URL: {full_url} (Attempt {attempt+1}/{retries}), params={params}")
                response = self.session.get(full_url, params=params)
                response.raise_for_status()
                return response.json(), None
            except requests.exceptions.RequestException as e:
                error_message = str(e)
                status_code = response.status_code if response else "N/A"
                self.logger.error(
                    f"Error calling {full_url}: {error_message} (Status: {status_code})",
                    extra={"error_category": "API Failure", "severity": "error"}
                )
                sleep_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                time.sleep(sleep_time)
        return None, "Max retries exceeded"

# ------------------------------
# File Extension -> Language Mapping
# ------------------------------
EXT_LANG_MAP = {
    ".py": "Python",
    ".java": "Java",
    ".js": "JavaScript",
    ".ts": "TypeScript",
    ".rb": "Ruby",
    ".cs": "C#",
    ".cpp": "C++",
    ".c": "C",
    ".go": "Go",
    ".php": "PHP",
    ".rs": "Rust",
    ".kt": "Kotlin",
    ".swift": "Swift",
    # Extend this map if needed.
}

# ------------------------------
# Filename -> Framework/Technology Mapping
# ------------------------------
FILE_FRAMEWORK_MAP = {
    "pom.xml": "Maven",
    "build.gradle": "Gradle",
    "package.json": "Node.js / npm",
    "requirements.txt": "Python (pip)",
    "Gemfile": "Ruby on Rails",
    "composer.json": "Composer (PHP)",
    "Cargo.toml": "Rust (Cargo)",
    "Makefile": "Make",
    # Extend this map if needed.
}

# ------------------------------
# Repo-Related API Calls
# ------------------------------
def get_repos(api_client: GenericAPIClient, project_key: str) -> list:
    """
    Retrieves the list of repositories (with IDs and URL information) for the given SPK.
    """
    endpoint = f"/projects/{project_key}/repos"
    params = {"limit": 1000}
    data, error = api_client.make_request(endpoint, params=params)
    if error:
        logger.error(
            f"Failed to get repos for project {project_key}: {error}",
            extra={"error_category": "API Failure", "severity": "error"}
        )
        return []
    return data.get("values", [])

def get_files_with_pagination(api_client: GenericAPIClient, project_key: str, repo_slug: str, limit: int = 25) -> list:
    """
    Retrieves ALL files in the repository by paging through the /files endpoint.
    """
    all_files = []
    start = 0
    while True:
        endpoint = f"/projects/{project_key}/repos/{repo_slug}/files"
        params = {"limit": limit, "start": start}
        response_data, error = api_client.make_request(endpoint, params=params)
        if error:
            logger.error(
                f"Failed to retrieve files for {project_key}/{repo_slug}, start={start}: {error}",
                extra={"error_category": "API Failure", "severity": "error"}
            )
            break
        # Expect the API to return a "values" list containing file paths.
        values = response_data.get("values", [])
        all_files.extend(values)
        if "nextPageStart" in response_data:
            start = response_data["nextPageStart"]
        else:
            break
    return all_files

# ------------------------------
# Analyzing File Paths
# ------------------------------
def analyze_file_list(file_list: list) -> dict:
    """
    Analyzes file paths to identify:
      - Languages (by file extensions)
      - Frameworks/technologies (by known filenames/folder hints)
      - Unique file extensions present in the repository
      - Total file count
    """
    languages = set()
    frameworks = set()
    unique_extensions = set()
    for file_path in file_list:
        base_name = os.path.basename(file_path)
        ext = os.path.splitext(base_name)[1].lower()
        if ext:
            unique_extensions.add(ext)
        # Language detection by extension
        if ext in EXT_LANG_MAP:
            languages.add(EXT_LANG_MAP[ext])
        # Framework detection by filename
        if base_name in FILE_FRAMEWORK_MAP:
            frameworks.add(FILE_FRAMEWORK_MAP[base_name])
        # Folder-based heuristics:
        if "node_modules" in file_path:
            frameworks.add("Node.js")
        if "venv" in file_path or "env" in file_path:
            frameworks.add("Python VirtualEnv")
        if "src/main/java" in file_path:
            languages.add("Java")
        if "src/main/resources" in file_path:
            frameworks.add("Spring")
    return {
        "languages": list(languages),
        "frameworks": list(frameworks),
        "unique_extensions": list(unique_extensions),
        "file_count": len(file_list)
    }

# ------------------------------
# Single Repository Analysis
# ------------------------------
def analyze_repository(api_client: GenericAPIClient, project_key: str, repo: dict) -> dict:
    """
    For the given repository, retrieves the file list (with pagination),
    then analyzes it to determine languages, frameworks, unique extensions, and total file count.
    Also captures the SPK, repo ID, and repo URL.
    """
    repo_slug = repo.get("slug")
    repo_id = repo.get("id")
    repo_name = repo.get("name", repo_slug)
    logger.info(f"Analyzing repository {project_key}/{repo_slug}, ID={repo_id}")

    # Attempt to capture the repository URL from the repo object's links.
    repo_links = repo.get("links", {})
    self_links = repo_links.get("self", [])
    if self_links and isinstance(self_links, list):
        repo_url = self_links[0].get("href", f"{api_client.base_url}/projects/{project_key}/repos/{repo_slug}")
    else:
        repo_url = f"{api_client.base_url}/projects/{project_key}/repos/{repo_slug}"

    # Retrieve all files from the repository.
    file_paths = get_files_with_pagination(api_client, project_key, repo_slug)
    file_analysis = analyze_file_list(file_paths)

    result = {
        "project_key": project_key,
        "repo_id": repo_id,
        "repo_slug": repo_slug,
        "repo_name": repo_name,
        "repo_url": repo_url,
        "languages": file_analysis["languages"],
        "frameworks": file_analysis["frameworks"],
        "unique_extensions": file_analysis["unique_extensions"],
        "file_count": file_analysis["file_count"]
    }
    return result

# ------------------------------
# Process Each SPK (Project)
# ------------------------------
def process_single_project(api_client: GenericAPIClient, project_key: str, max_repos: int) -> list:
    """
    Retrieves the repositories for the given SPK, then processes up to max_repos repositories.
    """
    repos = get_repos(api_client, project_key)
    if max_repos is not None:
        repos = repos[:max_repos]
    project_results = []
    for repo in repos:
        analysis = analyze_repository(api_client, project_key, repo)
        project_results.append(analysis)
    return project_results

def process_spks(input_excel: str, api_client: GenericAPIClient, max_repos: int) -> list:
    """
    Reads SPKs (project keys) from an Excel file, processes them in parallel,
    and returns aggregated analysis results.
    """
    wb = openpyxl.load_workbook(input_excel)
    sheet = wb.active
    spk_rows = list(sheet.iter_rows(min_row=2, values_only=True))
    all_results = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_project = {}
        for row in spk_rows:
            project_key = row[0]
            if not project_key:
                continue
            future = executor.submit(process_single_project, api_client, project_key, max_repos)
            future_to_project[future] = project_key

        for future in as_completed(future_to_project):
            project_key = future_to_project[future]
            results = future.result()
            all_results.extend(results)
            logger.info(f"Completed processing project {project_key}")
    return all_results

# ------------------------------
# Helpers for Output
# ------------------------------
def sanitize_map_objects(value):
    if isinstance(value, map):
        return list(value)
    return value

def sanitize_rows(rows):
    return [{k: sanitize_map_objects(v) for k, v in row.items()} for row in rows]

# ------------------------------
# Main Entry Point
# ------------------------------
def main():
    start_time = datetime.now()
    logger.info("MinimalRepoFileAnalyzer process started.")
    
    # Read SPKs from an Excel file
    input_excel = "WTMSPK.xlsx"  # Adjust filename/path as needed.
    
    # Bitbucket API configuration:
    BITBUCKET_BASE_URL = os.getenv("BITBUCKET_BASE_URL", "https://scm.example.com/rest/api/1.0")
    BITBUCKET_USERNAME = os.getenv("BITBUCKET_USERNAME", "")
    BITBUCKET_APP_PASSWORD = os.getenv("BITBUCKET_APP_PASSWORD", "")
    
    token_bucket = TokenBucket(capacity=75, tokens_per_second=5)
    api_client = GenericAPIClient(BITBUCKET_BASE_URL, token_bucket, auth=(BITBUCKET_USERNAME, BITBUCKET_APP_PASSWORD))
    
    results = process_spks(input_excel, api_client, MAX_REPOS_PER_SPK)
    sanitized = sanitize_rows(results)
    logger.info(f"Total repositories analyzed: {len(sanitized)}")
    
    # Create a pandas DataFrame and output the data into an Excel file.
    df = pd.DataFrame(sanitized)
    output_excel = "RepoAnalysisResults.xlsx"
    df.to_excel(output_excel, index=False)
    logger.info(f"Results written to {output_excel}")
    
    end_time = datetime.now()
    logger.info(f"Process completed in {str(end_time - start_time)}.")

if __name__ == "__main__":
    main()
